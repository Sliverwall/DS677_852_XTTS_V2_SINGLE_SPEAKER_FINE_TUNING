{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4d3f78",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Setup notes\n",
    "- If installing TTS package on a venv, install propper cuda enabled torch otherwise default torch will be installed, preventing cuda from being used.\n",
    "- Go to \"TTS\\tts\\layers\\tortoise\\arch_utils.py\" replace references of LogitWarper to LogitsProcessor\n",
    "- Go to \"TTS\\tts\\models\\xtts.py then to function get_compatible_checkpoint_state_dict. On line 714: checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"))[\"model\"]. Add the argument 'weights_only = False\": checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"), weights_only = False)[\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2b8b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''Imports'''\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from trainer.logging.wandb_logger import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c75a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "12.4\n",
      "True\n",
      "NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "'''Display device used'''\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)           \n",
    "print(torch.cuda.is_available())    \n",
    "print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099c65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set.\n"
     ]
    }
   ],
   "source": [
    "'''DOWNLOADS'''\n",
    "# Get XTTS files\n",
    "CHECKPOINT_PATH = './XTTS-files/'\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# DVAE files\n",
    "DVAE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(DVAE_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# DVAE download if not exists\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_LINK], CHECKPOINT_PATH, progress_bar=True)\n",
    "\n",
    "# XTTS v2.0 checkpoint\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# Transfer learning parameters. NOTE: Sets base model to use\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth\n",
    "\n",
    "# XTTS v2.0 download if not exists\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINT_PATH, progress_bar=True\n",
    "    )\n",
    "print(\"Paths set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08517a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 703 files in C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\datasets\\noramlized_personal_voice\n"
     ]
    }
   ],
   "source": [
    "'''DATA LOADING'''\n",
    "# Set lang\n",
    "LANGUAGE ='en'\n",
    "# Set to folder name that contains metadata.csv and wavs dir (with the .wav examples)\n",
    "DATASET= \"noramlized_personal_voice\"\n",
    "training_dir = f'./datasets/{DATASET}/' # change to folder w/ training examples\n",
    "\n",
    "# Dataset uses ljspeech format\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    meta_file_train=\"metadata.csv\", # metadata file w/ transcriptions\n",
    "    language=LANGUAGE,\n",
    "    path=training_dir\n",
    ")\n",
    "\n",
    "# Turn off eval split. Will evaluate manually\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_size=0.02 # Might change\n",
    ")\n",
    "\n",
    "\n",
    "'''MODIFY'''\n",
    "# Audio config\n",
    "audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000) \n",
    "\n",
    "# Speaker Reference: Match theses to the test sentences\n",
    "### Only need 1 speaker audio reference. Do not need to match voice to text\n",
    "SPEAKER_TEXT = [\n",
    "\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "    \"This cake is great. It's so delicious and moist.\"\n",
    "]\n",
    "SPEAKER_REFERENCE = f\"datasets/{DATASET}/wavs/chunk_0016.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set Model arguments'''\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=(22050*11), # Use sample rate units to define max condition length (for reference audio wav)\n",
    "    min_conditioning_length=(22050*3),\n",
    "    debug_loading_failures=True,\n",
    "    max_wav_length=(22050*11), # max 10 seconds\n",
    "    max_text_length=(22050*3), # min 3 seconds\n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  \n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    gpt_num_audio_tokens=1026, \n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c84e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set up configuration file'''\n",
    "'''TRAINING CONFIG'''\n",
    "OUT_PATH = './training_outputs/'\n",
    "\n",
    "RUN_NAME = f\"xttsv2_finetune_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "PROJECT_NAME = \"XTTS-v2 Finetune\"\n",
    "DASHBOARD_LOGGER = 'wandb'\n",
    "LOGGER_URI = None\n",
    "\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  \n",
    "\n",
    "BATCH_SIZE = 3 # 4 is common\n",
    "\n",
    "config = GPTTrainerConfig(\n",
    "    run_eval=True,\n",
    "    epochs = 10, # assuming you want to end training manually w/ keyboard interrupt\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    run_description=\"\"\"\n",
    "        GPT XTTS training\n",
    "        \"\"\",\n",
    "    dashboard_logger=DASHBOARD_LOGGER,\n",
    "    wandb_entity=None,\n",
    "    logger_uri=LOGGER_URI,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=0, # On Windows, num_loader_workers > 0 can break multiprocessing in PyTorch\n",
    "    eval_split_max_size=256, \n",
    "    print_step=50, \n",
    "    plot_step=100, \n",
    "    log_model_step=1000, \n",
    "    save_step=1000, # Needs to be an int\n",
    "    save_n_checkpoints=3, # Rotate last 3 checkpoints\n",
    "    save_checkpoints=True,\n",
    "    print_eval=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=5e-06,  \n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[ \n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[0],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE, \n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[1],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        }\n",
    "    ],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13eba573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 16\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DVAE weights restored from: ./XTTS-files/dvae.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msliverwall\u001b[0m (\u001b[33msliverwall-new-jersey-institute-of-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\wandb\\run-20250504_125022-epaofufn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune/runs/epaofufn' target=\"_blank\">xttsv2_finetune_20250504_1250</a></strong> to <a href='https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune' target=\"_blank\">https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune/runs/epaofufn' target=\"_blank\">https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune/runs/epaofufn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " > Model has 518442047 parameters\n"
     ]
    }
   ],
   "source": [
    "'''Set up Trainer'''\n",
    "# Init model \n",
    "model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "# Init Trainer\n",
    "GRAD_ACUMM_STEPS = 84 # Note: GRAD_ACUMM_STEPS * BATCH_SIZE = 252\n",
    "START_WITH_EVAL = True  \n",
    "\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None, # Change to model path if resuming\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=START_WITH_EVAL,\n",
    "        grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffe4fb20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Filtering invalid eval samples!!\n",
      " > Total eval samples after filtering: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.021762222051620483  (0.021762222051620483)\n",
      "     | > loss_mel_ce: 3.8056249618530273  (3.8056249618530273)\n",
      "     | > loss: 3.8273870944976807  (3.8273870944976807)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021934326738119125  (0.021934326738119125)\n",
      "     | > loss_mel_ce: 3.791006326675415  (3.791006326675415)\n",
      "     | > loss: 3.8129405975341797  (3.8129405975341797)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.02519591525197029  (0.023565120995044708)\n",
      "     | > loss_mel_ce: 4.018796443939209  (3.904901385307312)\n",
      "     | > loss: 4.043992519378662  (3.928466558456421)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.021754387766122818  (0.022961543252070744)\n",
      "     | > loss_mel_ce: 3.4101994037628174  (3.7400007247924805)\n",
      "     | > loss: 3.4319539070129395  (3.7629623413085938)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.0275751780718565  (0.024114951957017183)\n",
      "     | > loss_mel_ce: 4.1670823097229  (3.8467711210250854)\n",
      "     | > loss: 4.194657325744629  (3.8708860874176025)\n",
      "\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time: 0.0227736234664917 \u001b[0m(+0)\n",
      "     | > avg_loss_text_ce: 0.024114951957017183 \u001b[0m(+0)\n",
      "     | > avg_loss_mel_ce: 3.8467711210250854 \u001b[0m(+0)\n",
      "     | > avg_loss: 3.8708860874176025 \u001b[0m(+0)\n",
      "\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 1/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 12:50:30) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Sampling by language: dict_keys(['en'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 12:50:36 -- STEP: 0/230 -- GLOBAL_STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.02736741304397583  (0.02736741304397583)\n",
      "     | > loss_mel_ce: 3.9518797397613525  (3.9518797397613525)\n",
      "     | > loss: 0.047371990978717804  (0.047371990978717804)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 6.2011  (6.2011120319366455)\n",
      "     | > loader_time: 0.0203  (0.020256996154785156)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\training_outputs\\xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c)... Done. 0.0s\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 12:57:19 -- STEP: 50/230 -- GLOBAL_STEP: 50\u001b[0m\n",
      "     | > loss_text_ce: 0.02770291455090046  (0.025397119112312794)\n",
      "     | > loss_mel_ce: 3.9957215785980225  (3.9501606369018556)\n",
      "     | > loss: 0.04789791256189346  (0.04732806928455831)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.3159  (2.897572021484375)\n",
      "     | > loader_time: 0.0254  (0.026508593559265138)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 13:05:28 -- STEP: 100/230 -- GLOBAL_STEP: 100\u001b[0m\n",
      "     | > loss_text_ce: 0.024643786251544952  (0.025527611430734388)\n",
      "     | > loss_mel_ce: 3.7815282344818115  (3.886800329685211)\n",
      "     | > loss: 0.0453115738928318  (0.04657533343881368)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 10.0038  (5.192517192363739)\n",
      "     | > loader_time: 0.0317  (0.028544840812683107)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 13:14:51 -- STEP: 150/230 -- GLOBAL_STEP: 150\u001b[0m\n",
      "     | > loss_text_ce: 0.02890883944928646  (0.02532537507514159)\n",
      "     | > loss_mel_ce: 4.010008811950684  (3.897107988993327)\n",
      "     | > loss: 0.04808235168457031  (0.04669563606381416)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 12.9561  (6.835044507980347)\n",
      "     | > loader_time: 0.0159  (0.030696511268615723)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error loading ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav (<class 'AssertionError'>, AssertionError('UNK token found in  Another useful contribution is that deep player functions are very fast as many key operations are coded in C++.\\n -> [en]another useful contribution is that deep player functions are very fast as many key operations are coded in c.'), <traceback object at 0x00000202CE227880>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 13:23:41 -- STEP: 200/230 -- GLOBAL_STEP: 200\u001b[0m\n",
      "     | > loss_text_ce: 0.02432379499077797  (0.025138790719211112)\n",
      "     | > loss_mel_ce: 4.0928053855896  (3.873010665178299)\n",
      "     | > loss: 0.049013447016477585  (0.0464065419882536)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 10.0056  (7.49384029507637)\n",
      "     | > loader_time: 0.0295  (0.030196043252944945)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.02171185426414013  (0.02171185426414013)\n",
      "     | > loss_mel_ce: 3.6262032985687256  (3.6262032985687256)\n",
      "     | > loss: 3.6479151248931885  (3.6479151248931885)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021888677030801773  (0.021888677030801773)\n",
      "     | > loss_mel_ce: 3.6042394638061523  (3.6042394638061523)\n",
      "     | > loss: 3.6261281967163086  (3.6261281967163086)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.02515198476612568  (0.023520330898463726)\n",
      "     | > loss_mel_ce: 3.7666802406311035  (3.685459852218628)\n",
      "     | > loss: 3.791832208633423  (3.7089802026748657)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.02175145037472248  (0.022930704057216644)\n",
      "     | > loss_mel_ce: 3.1950371265411377  (3.5219856103261313)\n",
      "     | > loss: 3.2167885303497314  (3.544916311899821)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.02749786339700222  (0.024072493892163038)\n",
      "     | > loss_mel_ce: 3.901393413543701  (3.6168375611305237)\n",
      "     | > loss: 3.928891181945801  (3.640910029411316)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.01312243938446045 \u001b[0m(-0.00965118408203125)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.024072493892163038 \u001b[0m(-4.245806485414505e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.6168375611305237 \u001b[0m(-0.22993355989456177)\n",
      "     | > avg_loss:\u001b[92m 3.640910029411316 \u001b[0m(-0.22997605800628662)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_230.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 2/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 13:29:09) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 13:33:11 -- STEP: 20/230 -- GLOBAL_STEP: 250\u001b[0m\n",
      "     | > loss_text_ce: 0.024602022022008896  (0.02593605639412999)\n",
      "     | > loss_mel_ce: 3.70546817779541  (3.766446256637573)\n",
      "     | > loss: 0.04440559819340706  (0.045147409290075304)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.2492  (3.0704188108444215)\n",
      "     | > loader_time: 0.0339  (0.020787596702575684)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 13:43:22 -- STEP: 70/230 -- GLOBAL_STEP: 300\u001b[0m\n",
      "     | > loss_text_ce: 0.026942411437630653  (0.025339128343122345)\n",
      "     | > loss_mel_ce: 3.8788211345672607  (3.737304721559797)\n",
      "     | > loss: 0.04649718850851059  (0.04479337989219597)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.7844  (3.1992084400994436)\n",
      "     | > loader_time: 0.017  (0.02026380470820836)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 13:52:43 -- STEP: 120/230 -- GLOBAL_STEP: 350\u001b[0m\n",
      "     | > loss_text_ce: 0.023069674149155617  (0.02499198109532396)\n",
      "     | > loss_mel_ce: 3.5164153575897217  (3.7013941486676534)\n",
      "     | > loss: 0.042136725038290024  (0.04436174038176734)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.0052  (3.152536761760712)\n",
      "     | > loader_time: 0.0158  (0.01924025019009908)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error loading ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav (<class 'AssertionError'>, AssertionError('UNK token found in  You can see that the algorithm converged and obtained an estimate of Î¼ and Ïƒ. We can also try to\\n -> [en]you can see that the algorithm converged and obtained an estimate of Î¼ and . we can also try to'), <traceback object at 0x00000202CE227740>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 14:01:46 -- STEP: 170/230 -- GLOBAL_STEP: 400\u001b[0m\n",
      "     | > loss_text_ce: 0.027420639991760254  (0.02499215723398853)\n",
      "     | > loss_mel_ce: 3.7945311069488525  (3.696958381989423)\n",
      "     | > loss: 0.04549942910671234  (0.04430893571937785)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.6851  (3.8320623173433197)\n",
      "     | > loader_time: 0.0311  (0.019707243582781624)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 14:11:31 -- STEP: 220/230 -- GLOBAL_STEP: 450\u001b[0m\n",
      "     | > loss_text_ce: 0.02541688084602356  (0.025006731362505388)\n",
      "     | > loss_mel_ce: 3.582221269607544  (3.688327723199671)\n",
      "     | > loss: 0.042948074638843536  (0.04420636330138554)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.9321  (3.995974132147703)\n",
      "     | > loader_time: 0.015  (0.020250595699657092)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.02164589613676071  (0.02164589613676071)\n",
      "     | > loss_mel_ce: 3.5339882373809814  (3.5339882373809814)\n",
      "     | > loss: 3.555634021759033  (3.555634021759033)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021839689463377  (0.021839689463377)\n",
      "     | > loss_mel_ce: 3.4866974353790283  (3.4866974353790283)\n",
      "     | > loss: 3.5085370540618896  (3.5085370540618896)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.025081852450966835  (0.023460770957171917)\n",
      "     | > loss_mel_ce: 3.681941032409668  (3.584319233894348)\n",
      "     | > loss: 3.7070229053497314  (3.6077799797058105)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.021735576912760735  (0.022885706275701523)\n",
      "     | > loss_mel_ce: 3.1218206882476807  (3.4301530520121255)\n",
      "     | > loss: 3.1435563564300537  (3.453038771947225)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.02741544507443905  (0.024018140975385904)\n",
      "     | > loss_mel_ce: 3.7867555618286133  (3.5193036794662476)\n",
      "     | > loss: 3.814171075820923  (3.5433218479156494)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.015694618225097656 \u001b[0m(+0.002572178840637207)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.024018140975385904 \u001b[0m(-5.435291677713394e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.5193036794662476 \u001b[0m(-0.09753388166427612)\n",
      "     | > avg_loss:\u001b[92m 3.5433218479156494 \u001b[0m(-0.0975881814956665)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_460.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 3/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 14:13:40) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 14:20:47 -- STEP: 40/230 -- GLOBAL_STEP: 500\u001b[0m\n",
      "     | > loss_text_ce: 0.026652071624994278  (0.024855676898732783)\n",
      "     | > loss_mel_ce: 3.651787519454956  (3.6210572481155396)\n",
      "     | > loss: 0.043790947645902634  (0.04340372625738381)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.8766  (2.786069840192795)\n",
      "     | > loader_time: 0.017  (0.015826195478439335)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 14:29:36 -- STEP: 90/230 -- GLOBAL_STEP: 550\u001b[0m\n",
      "     | > loss_text_ce: 0.020934129133820534  (0.02521971505549219)\n",
      "     | > loss_mel_ce: 3.3203125  (3.6411246723598905)\n",
      "     | > loss: 0.039776746183633804  (0.0436469577666786)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.0987  (2.8568178441789414)\n",
      "     | > loader_time: 0.0158  (0.015577268600463868)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 14:38:20 -- STEP: 140/230 -- GLOBAL_STEP: 600\u001b[0m\n",
      "     | > loss_text_ce: 0.028105083853006363  (0.02516503373959235)\n",
      "     | > loss_mel_ce: 3.5037200450897217  (3.637849572726658)\n",
      "     | > loss: 0.04204553738236427  (0.04360731744340488)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.0286  (2.906244703701564)\n",
      "     | > loader_time: 0.0129  (0.015637636184692383)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 14:48:09 -- STEP: 190/230 -- GLOBAL_STEP: 650\u001b[0m\n",
      "     | > loss_text_ce: 0.02202760800719261  (0.025075115959503146)\n",
      "     | > loss_mel_ce: 3.9938790798187256  (3.632510040935717)\n",
      "     | > loss: 0.047808416187763214  (0.043542681242290306)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.1356  (3.0220538227181675)\n",
      "     | > loader_time: 0.0145  (0.015413416059393632)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.021546468138694763  (0.021546468138694763)\n",
      "     | > loss_mel_ce: 3.474316120147705  (3.474316120147705)\n",
      "     | > loss: 3.4958624839782715  (3.4958624839782715)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021786266937851906  (0.021786266937851906)\n",
      "     | > loss_mel_ce: 3.4124836921691895  (3.4124836921691895)\n",
      "     | > loss: 3.434269905090332  (3.434269905090332)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.024986036121845245  (0.023386151529848576)\n",
      "     | > loss_mel_ce: 3.6499202251434326  (3.531201958656311)\n",
      "     | > loss: 3.6749062538146973  (3.5545880794525146)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.021692737936973572  (0.022821680332223575)\n",
      "     | > loss_mel_ce: 3.0782222747802734  (3.380208730697632)\n",
      "     | > loss: 3.099915027618408  (3.4030303955078125)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.027331089600920677  (0.02394903264939785)\n",
      "     | > loss_mel_ce: 3.73539662361145  (3.4690057039260864)\n",
      "     | > loss: 3.762727737426758  (3.492954730987549)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.015017509460449219 \u001b[0m(-0.0006771087646484375)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.02394903264939785 \u001b[0m(-6.910832598805428e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.4690057039260864 \u001b[0m(-0.05029797554016113)\n",
      "     | > avg_loss:\u001b[92m 3.492954730987549 \u001b[0m(-0.050367116928100586)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_690.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 4/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 14:55:57) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 14:58:40 -- STEP: 10/230 -- GLOBAL_STEP: 700\u001b[0m\n",
      "     | > loss_text_ce: 0.025397874414920807  (0.024878767505288123)\n",
      "     | > loss_mel_ce: 3.4831349849700928  (3.5722805976867678)\n",
      "     | > loss: 0.04176824912428856  (0.042823326960206035)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.3223  (4.431906676292419)\n",
      "     | > loader_time: 0.0141  (0.01723039150238037)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 15:07:10 -- STEP: 60/230 -- GLOBAL_STEP: 750\u001b[0m\n",
      "     | > loss_text_ce: 0.02430034801363945  (0.024692826842268308)\n",
      "     | > loss_mel_ce: 3.7291338443756104  (3.5705307801564534)\n",
      "     | > loss: 0.04468373954296112  (0.042800281755626196)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.5908  (3.0129100084304805)\n",
      "     | > loader_time: 0.015  (0.016045248508453364)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 15:17:22 -- STEP: 110/230 -- GLOBAL_STEP: 800\u001b[0m\n",
      "     | > loss_text_ce: 0.02265559323132038  (0.02464684805070812)\n",
      "     | > loss_mel_ce: 3.2587788105010986  (3.5692092765461316)\n",
      "     | > loss: 0.039064694195985794  (0.04278400225395506)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.452  (4.038204546408218)\n",
      "     | > loader_time: 0.013  (0.017167594216086653)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 15:34:29 -- STEP: 160/230 -- GLOBAL_STEP: 850\u001b[0m\n",
      "     | > loss_text_ce: 0.022869858890771866  (0.0247204422717914)\n",
      "     | > loss_mel_ce: 3.276261329650879  (3.56926601678133)\n",
      "     | > loss: 0.03927537053823471  (0.042785553936846556)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.4996  (6.637287424504754)\n",
      "     | > loader_time: 0.014  (0.016998481750488282)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 15:57:16 -- STEP: 210/230 -- GLOBAL_STEP: 900\u001b[0m\n",
      "     | > loss_text_ce: 0.02160361409187317  (0.02463118647713037)\n",
      "     | > loss_mel_ce: 3.6851212978363037  (3.554648460660662)\n",
      "     | > loss: 0.044127676635980606  (0.04261047290194601)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.6467  (9.544260495049608)\n",
      "     | > loader_time: 0.0151  (0.016682209287370957)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.0214256402105093  (0.0214256402105093)\n",
      "     | > loss_mel_ce: 3.428621530532837  (3.428621530532837)\n",
      "     | > loss: 3.450047254562378  (3.450047254562378)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021728135645389557  (0.021728135645389557)\n",
      "     | > loss_mel_ce: 3.369685649871826  (3.369685649871826)\n",
      "     | > loss: 3.391413688659668  (3.391413688659668)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.02487998455762863  (0.023304060101509094)\n",
      "     | > loss_mel_ce: 3.622671604156494  (3.49617862701416)\n",
      "     | > loss: 3.6475515365600586  (3.5194826126098633)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.021632373332977295  (0.02274683117866516)\n",
      "     | > loss_mel_ce: 3.0439162254333496  (3.34542449315389)\n",
      "     | > loss: 3.0655486583709717  (3.368171294530233)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.027248937636613846  (0.023872357793152332)\n",
      "     | > loss_mel_ce: 3.6972897052764893  (3.43339079618454)\n",
      "     | > loss: 3.724538564682007  (3.4572631120681763)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.016025543212890625 \u001b[0m(+0.0010080337524414062)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.023872357793152332 \u001b[0m(-7.667485624551773e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.43339079618454 \u001b[0m(-0.03561490774154663)\n",
      "     | > avg_loss:\u001b[92m 3.4572631120681763 \u001b[0m(-0.03569161891937256)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_920.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 5/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 16:03:29) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 16:11:50 -- STEP: 30/230 -- GLOBAL_STEP: 950\u001b[0m\n",
      "     | > loss_text_ce: 0.027785643935203552  (0.025221229158341886)\n",
      "     | > loss_mel_ce: 3.2591328620910645  (3.5236709912618003)\n",
      "     | > loss: 0.03912997990846634  (0.042248717571298285)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 6.6432  (4.568212175369263)\n",
      "     | > loader_time: 0.0169  (0.016440550486246753)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 16:27:53 -- STEP: 80/230 -- GLOBAL_STEP: 1000\u001b[0m\n",
      "     | > loss_text_ce: 0.02577793039381504  (0.024781629256904126)\n",
      "     | > loss_mel_ce: 3.5102763175964355  (3.5418633997440336)\n",
      "     | > loss: 0.042095884680747986  (0.04246006067842245)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.1617  (5.2335094720125195)\n",
      "     | > loader_time: 0.0156  (0.015938907861709588)\n",
      "\n",
      "\n",
      " > CHECKPOINT : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\checkpoint_1000.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\training_outputs\\xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c)... Done. 39.4s\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 16:38:46 -- STEP: 130/230 -- GLOBAL_STEP: 1050\u001b[0m\n",
      "     | > loss_text_ce: 0.025388605892658234  (0.024797531174352537)\n",
      "     | > loss_mel_ce: 3.5731875896453857  (3.517562464567331)\n",
      "     | > loss: 0.04284019395709038  (0.04217095306286445)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.0128  (4.472696685791018)\n",
      "     | > loader_time: 0.0145  (0.02126736274132361)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n",
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 16:47:48 -- STEP: 180/230 -- GLOBAL_STEP: 1100\u001b[0m\n",
      "     | > loss_text_ce: 0.026017239317297935  (0.02480155466538336)\n",
      "     | > loss_mel_ce: 3.395176887512207  (3.5048231535487706)\n",
      "     | > loss: 0.040728501975536346  (0.042019342486229204)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.0859  (4.1044195175170906)\n",
      "     | > loader_time: 0.0156  (0.019807656606038414)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.021300693973898888  (0.021300693973898888)\n",
      "     | > loss_mel_ce: 3.392547845840454  (3.392547845840454)\n",
      "     | > loss: 3.413848638534546  (3.413848638534546)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021670367568731308  (0.021670367568731308)\n",
      "     | > loss_mel_ce: 3.340406894683838  (3.340406894683838)\n",
      "     | > loss: 3.362077236175537  (3.362077236175537)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.024777373299002647  (0.023223870433866978)\n",
      "     | > loss_mel_ce: 3.5950794219970703  (3.467743158340454)\n",
      "     | > loss: 3.619856834411621  (3.490967035293579)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.0215664803981781  (0.022671407088637352)\n",
      "     | > loss_mel_ce: 3.013906717300415  (3.316464344660441)\n",
      "     | > loss: 3.035473108291626  (3.339135726292928)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.027173331007361412  (0.023796888068318367)\n",
      "     | > loss_mel_ce: 3.671267509460449  (3.405165135860443)\n",
      "     | > loss: 3.6984407901763916  (3.428961992263794)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.01534879207611084 \u001b[0m(-0.0006767511367797852)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.023796888068318367 \u001b[0m(-7.54697248339653e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.405165135860443 \u001b[0m(-0.02822566032409668)\n",
      "     | > avg_loss:\u001b[92m 3.428961992263794 \u001b[0m(-0.028301119804382324)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_1150.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 6/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 17:00:08) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 17:00:22 -- STEP: 0/230 -- GLOBAL_STEP: 1150\u001b[0m\n",
      "     | > loss_text_ce: 0.02570449374616146  (0.02570449374616146)\n",
      "     | > loss_mel_ce: 3.381763219833374  (3.381763219833374)\n",
      "     | > loss: 0.040565092116594315  (0.040565092116594315)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.2972  (3.2971622943878174)\n",
      "     | > loader_time: 0.0535  (0.0534818172454834)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 17:38:47 -- STEP: 50/230 -- GLOBAL_STEP: 1200\u001b[0m\n",
      "     | > loss_text_ce: 0.021381745114922523  (0.024566980339586736)\n",
      "     | > loss_mel_ce: 3.751950979232788  (3.460927472114563)\n",
      "     | > loss: 0.044920630753040314  (0.04149398230016233)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.6575  (36.71710210800171)\n",
      "     | > loader_time: 0.016  (0.015192112922668456)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 17:47:14 -- STEP: 100/230 -- GLOBAL_STEP: 1250\u001b[0m\n",
      "     | > loss_text_ce: 0.02580801211297512  (0.024547864757478235)\n",
      "     | > loss_mel_ce: 3.4040565490722656  (3.4696395659446715)\n",
      "     | > loss: 0.04083172231912613  (0.04159747023135424)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.6232  (19.781923756599426)\n",
      "     | > loader_time: 0.0138  (0.015003643035888671)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 17:55:39 -- STEP: 150/230 -- GLOBAL_STEP: 1300\u001b[0m\n",
      "     | > loss_text_ce: 0.02818452939391136  (0.0247337241222461)\n",
      "     | > loss_mel_ce: 3.487704277038574  (3.477954489390055)\n",
      "     | > loss: 0.0418558195233345  (0.04169866998990377)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.7975  (14.102913273175563)\n",
      "     | > loader_time: 0.0145  (0.014870279630025228)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 18:05:41 -- STEP: 200/230 -- GLOBAL_STEP: 1350\u001b[0m\n",
      "     | > loss_text_ce: 0.02705996297299862  (0.024762125415727487)\n",
      "     | > loss_mel_ce: 3.715608835220337  (3.4769335424900056)\n",
      "     | > loss: 0.044555582106113434  (0.04168685395270586)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 28.1773  (11.593655514717106)\n",
      "     | > loader_time: 0.0333  (0.014937872886657716)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.02118760533630848  (0.02118760533630848)\n",
      "     | > loss_mel_ce: 3.36683988571167  (3.36683988571167)\n",
      "     | > loss: 3.3880274295806885  (3.3880274295806885)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021616017445921898  (0.021616017445921898)\n",
      "     | > loss_mel_ce: 3.318708896636963  (3.318708896636963)\n",
      "     | > loss: 3.340324878692627  (3.340324878692627)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.024686098098754883  (0.02315105777233839)\n",
      "     | > loss_mel_ce: 3.5688610076904297  (3.4437849521636963)\n",
      "     | > loss: 3.5935471057891846  (3.4669359922409058)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.021502485498785973  (0.02260153368115425)\n",
      "     | > loss_mel_ce: 2.9925994873046875  (3.2933897972106934)\n",
      "     | > loss: 3.014101982116699  (3.3159913221995034)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.02710093930363655  (0.023726385086774826)\n",
      "     | > loss_mel_ce: 3.648397922515869  (3.3821418285369873)\n",
      "     | > loss: 3.6754989624023438  (3.4058682322502136)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.013132691383361816 \u001b[0m(-0.0022161006927490234)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.023726385086774826 \u001b[0m(-7.050298154354095e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.3821418285369873 \u001b[0m(-0.02302330732345581)\n",
      "     | > avg_loss:\u001b[92m 3.4058682322502136 \u001b[0m(-0.023093760013580322)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_1380.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 7/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 18:15:56) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 18:19:19 -- STEP: 20/230 -- GLOBAL_STEP: 1400\u001b[0m\n",
      "     | > loss_text_ce: 0.028840254992246628  (0.024920130241662265)\n",
      "     | > loss_mel_ce: 3.3603217601776123  (3.418428194522858)\n",
      "     | > loss: 0.04034716635942459  (0.04099224228411913)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.814  (2.732638144493103)\n",
      "     | > loader_time: 0.0149  (0.01477283239364624)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 18:36:53 -- STEP: 70/230 -- GLOBAL_STEP: 1450\u001b[0m\n",
      "     | > loss_text_ce: 0.026961294934153557  (0.02493534314313105)\n",
      "     | > loss_mel_ce: 3.1085729598999023  (3.397600906235831)\n",
      "     | > loss: 0.03732778877019882  (0.04074447974562645)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.6804  (5.856028996195111)\n",
      "     | > loader_time: 0.012  (0.034839085170200894)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 18:47:59 -- STEP: 120/230 -- GLOBAL_STEP: 1500\u001b[0m\n",
      "     | > loss_text_ce: 0.022760486230254173  (0.02490349393337965)\n",
      "     | > loss_mel_ce: 3.5500545501708984  (3.390504084030787)\n",
      "     | > loss: 0.0425335131585598  (0.04065961465239525)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.0519  (5.3779821554819724)\n",
      "     | > loader_time: 0.0153  (0.027723751465479538)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 18:55:50 -- STEP: 170/230 -- GLOBAL_STEP: 1550\u001b[0m\n",
      "     | > loss_text_ce: 0.025320759043097496  (0.024805147409000812)\n",
      "     | > loss_mel_ce: 3.4310619831085205  (3.3968994673560635)\n",
      "     | > loss: 0.04114741459488869  (0.04073457941412926)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.116  (4.631322923828573)\n",
      "     | > loader_time: 0.0119  (0.023954008607303394)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 19:04:22 -- STEP: 220/230 -- GLOBAL_STEP: 1600\u001b[0m\n",
      "     | > loss_text_ce: 0.022752178832888603  (0.02474394377151673)\n",
      "     | > loss_mel_ce: 3.4637115001678467  (3.4089270234107962)\n",
      "     | > loss: 0.04150552302598953  (0.04087703598832544)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.8401  (4.250323708490889)\n",
      "     | > loader_time: 0.013  (0.021864431554620915)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.021080613136291504  (0.021080613136291504)\n",
      "     | > loss_mel_ce: 3.344940662384033  (3.344940662384033)\n",
      "     | > loss: 3.366021156311035  (3.366021156311035)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021565420553088188  (0.021565420553088188)\n",
      "     | > loss_mel_ce: 3.2996513843536377  (3.2996513843536377)\n",
      "     | > loss: 3.3212168216705322  (3.3212168216705322)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.024595970287919044  (0.023080695420503616)\n",
      "     | > loss_mel_ce: 3.5479743480682373  (3.4238128662109375)\n",
      "     | > loss: 3.572570323944092  (3.446893572807312)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.02144375443458557  (0.0225350484251976)\n",
      "     | > loss_mel_ce: 2.974576473236084  (3.2740674018859863)\n",
      "     | > loss: 2.9960203170776367  (3.296602487564087)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.027025172486901283  (0.023657579440623522)\n",
      "     | > loss_mel_ce: 3.6269421577453613  (3.36228609085083)\n",
      "     | > loss: 3.6539673805236816  (3.3859437108039856)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.01638394594192505 \u001b[0m(+0.0032512545585632324)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.023657579440623522 \u001b[0m(-6.880564615130424e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.36228609085083 \u001b[0m(-0.019855737686157227)\n",
      "     | > avg_loss:\u001b[92m 3.3859437108039856 \u001b[0m(-0.019924521446228027)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_1610.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 8/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 19:06:17) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n",
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 19:13:35 -- STEP: 40/230 -- GLOBAL_STEP: 1650\u001b[0m\n",
      "     | > loss_text_ce: 0.02771531604230404  (0.02523341653868556)\n",
      "     | > loss_mel_ce: 3.492427349090576  (3.4282394111156465)\n",
      "     | > loss: 0.041906461119651794  (0.041112772654742)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.7807  (2.899973380565643)\n",
      "     | > loader_time: 0.0153  (0.014112502336502075)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 19:25:37 -- STEP: 90/230 -- GLOBAL_STEP: 1700\u001b[0m\n",
      "     | > loss_text_ce: 0.02241230010986328  (0.025063377805054188)\n",
      "     | > loss_mel_ce: 3.4081647396087646  (3.425644490453932)\n",
      "     | > loss: 0.04084020480513573  (0.04107985616558126)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.8391  (3.058094284269545)\n",
      "     | > loader_time: 0.013  (0.014034957355923123)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 19:34:45 -- STEP: 140/230 -- GLOBAL_STEP: 1750\u001b[0m\n",
      "     | > loss_text_ce: 0.025143105536699295  (0.024848589793379817)\n",
      "     | > loss_mel_ce: 3.331425189971924  (3.42670236996242)\n",
      "     | > loss: 0.039959147572517395  (0.041089892972792884)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.0654  (3.012140360900334)\n",
      "     | > loader_time: 0.013  (0.013746367182050433)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 19:43:44 -- STEP: 190/230 -- GLOBAL_STEP: 1800\u001b[0m\n",
      "     | > loss_text_ce: 0.021639417856931686  (0.024664794261518283)\n",
      "     | > loss_mel_ce: 3.269710063934326  (3.411451092519258)\n",
      "     | > loss: 0.03918273001909256  (0.04090614209049628)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.934  (2.9694323414250436)\n",
      "     | > loader_time: 0.012  (0.01363054827639931)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.020982103422284126  (0.020982103422284126)\n",
      "     | > loss_mel_ce: 3.323807716369629  (3.323807716369629)\n",
      "     | > loss: 3.344789743423462  (3.344789743423462)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021515557542443275  (0.021515557542443275)\n",
      "     | > loss_mel_ce: 3.2812423706054688  (3.2812423706054688)\n",
      "     | > loss: 3.302757978439331  (3.302757978439331)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.024505384266376495  (0.023010470904409885)\n",
      "     | > loss_mel_ce: 3.531176805496216  (3.4062095880508423)\n",
      "     | > loss: 3.5556821823120117  (3.4292200803756714)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.021390024572610855  (0.022470322127143543)\n",
      "     | > loss_mel_ce: 2.957737445831299  (3.256718873977661)\n",
      "     | > loss: 2.9791274070739746  (3.279189189275106)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.026948804035782814  (0.02358994260430336)\n",
      "     | > loss_mel_ce: 3.6081383228302  (3.344573736190796)\n",
      "     | > loss: 3.635087013244629  (3.3681636452674866)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[92m 0.01584911346435547 \u001b[0m(-0.0005348324775695801)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.02358994260430336 \u001b[0m(-6.763683632016182e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.344573736190796 \u001b[0m(-0.01771235466003418)\n",
      "     | > avg_loss:\u001b[92m 3.3681636452674866 \u001b[0m(-0.017780065536499023)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_1840.pth\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 9/10\u001b[0m\n",
      " --> ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\n",
      "\n",
      "\u001b[1m > TRAINING (2025-05-04 19:55:10) \u001b[0m\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 19:57:05 -- STEP: 10/230 -- GLOBAL_STEP: 1850\u001b[0m\n",
      "     | > loss_text_ce: 0.024348394945263863  (0.02541128396987915)\n",
      "     | > loss_mel_ce: 3.4443140029907227  (3.401384687423706)\n",
      "     | > loss: 0.04129360243678093  (0.040795191377401355)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.937  (2.759142756462097)\n",
      "     | > loader_time: 0.0131  (0.014509010314941406)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 20:09:44 -- STEP: 60/230 -- GLOBAL_STEP: 1900\u001b[0m\n",
      "     | > loss_text_ce: 0.027426160871982574  (0.024837872106581928)\n",
      "     | > loss_mel_ce: 3.369276523590088  (3.385108252366384)\n",
      "     | > loss: 0.040436938405036926  (0.040594597719609736)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 3.1067  (3.99272266626358)\n",
      "     | > loader_time: 0.014  (0.014822924137115478)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 20:20:34 -- STEP: 110/230 -- GLOBAL_STEP: 1950\u001b[0m\n",
      "     | > loss_text_ce: 0.023154711350798607  (0.0247301035137339)\n",
      "     | > loss_mel_ce: 3.3581361770629883  (3.3849914052269674)\n",
      "     | > loss: 0.04025346413254738  (0.04059192351996897)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.7775  (3.7092055169018834)\n",
      "     | > loader_time: 0.0146  (0.014827431331981313)\n",
      "\n",
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 20:36:06 -- STEP: 160/230 -- GLOBAL_STEP: 2000\u001b[0m\n",
      "     | > loss_text_ce: 0.027973951771855354  (0.02474205429898574)\n",
      "     | > loss_mel_ce: 3.290245771408081  (3.3840340167284007)\n",
      "     | > loss: 0.039502616971731186  (0.0405806682538241)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 2.7167  (5.388935492932797)\n",
      "     | > loader_time: 0.0148  (0.015680527687072753)\n",
      "\n",
      "\n",
      " > CHECKPOINT : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\checkpoint_2000.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\training_outputs\\xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c)... Done. 44.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0365.wav because it was already ignored before !!\n",
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n",
      "Ignoring sample ./datasets/noramlized_personal_voice/wavs\\chunk_0658.wav because it was already ignored before !!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m   --> TIME: 2025-05-04 20:50:19 -- STEP: 210/230 -- GLOBAL_STEP: 2050\u001b[0m\n",
      "     | > loss_text_ce: 0.02499692142009735  (0.02472384764502445)\n",
      "     | > loss_mel_ce: 3.545748710632324  (3.39621425583249)\n",
      "     | > loss: 0.04250887781381607  (0.04072545429780367)\n",
      "     | > current_lr: 5e-06 \n",
      "     | > step_time: 11.4648  (6.09997319493975)\n",
      "     | > loader_time: 0.0159  (0.019097761880783815)\n",
      "\n",
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n",
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.020889604464173317  (0.020889604464173317)\n",
      "     | > loss_mel_ce: 3.302748441696167  (3.302748441696167)\n",
      "     | > loss: 3.3236379623413086  (3.3236379623413086)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.021463457494974136  (0.021463457494974136)\n",
      "     | > loss_mel_ce: 3.26389217376709  (3.26389217376709)\n",
      "     | > loss: 3.285355567932129  (3.285355567932129)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.024410953745245934  (0.022937205620110035)\n",
      "     | > loss_mel_ce: 3.5183804035186768  (3.3911362886428833)\n",
      "     | > loss: 3.5427913665771484  (3.4140734672546387)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.02133907936513424  (0.022404496868451435)\n",
      "     | > loss_mel_ce: 2.9414894580841064  (3.2412540117899575)\n",
      "     | > loss: 2.9628286361694336  (3.2636585235595703)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.026867976412177086  (0.02352036675438285)\n",
      "     | > loss_mel_ce: 3.59354305267334  (3.3293262720108032)\n",
      "     | > loss: 3.620410919189453  (3.352846622467041)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  \u001b[1m--> EVAL PERFORMANCE\u001b[0m\n",
      "     | > avg_loader_time:\u001b[91m 0.01636296510696411 \u001b[0m(+0.0005138516426086426)\n",
      "     | > avg_loss_text_ce:\u001b[92m 0.02352036675438285 \u001b[0m(-6.957584992051125e-05)\n",
      "     | > avg_loss_mel_ce:\u001b[92m 3.3293262720108032 \u001b[0m(-0.015247464179992676)\n",
      "     | > avg_loss:\u001b[92m 3.352846622467041 \u001b[0m(-0.015317022800445557)\n",
      "\n",
      " > BEST MODEL : ./training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\\best_model_2070.pth\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>EvalStats/avg_loader_time</td><td>â–ˆâ–â–ƒâ–‚â–ƒâ–ƒâ–â–ƒâ–ƒ</td></tr><tr><td>EvalStats/avg_loss</td><td>â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–</td></tr><tr><td>EvalStats/avg_loss_mel_ce</td><td>â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–â–</td></tr><tr><td>EvalStats/avg_loss_text_ce</td><td>â–ˆâ–‡â–‡â–†â–…â–„â–ƒâ–‚â–</td></tr><tr><td>TrainEpochStats/avg_grad_norm</td><td>â–â–â–â–â–â–â–â–</td></tr><tr><td>TrainEpochStats/avg_loader_time</td><td>â–ˆâ–„â–‚â–‚â–ƒâ–‚â–„â–</td></tr><tr><td>TrainEpochStats/avg_loss</td><td>â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–</td></tr><tr><td>TrainEpochStats/avg_loss_mel_ce</td><td>â–ˆâ–…â–„â–ƒâ–‚â–‚â–â–</td></tr><tr><td>TrainEpochStats/avg_loss_text_ce</td><td>â–ˆâ–†â–†â–ƒâ–„â–„â–ƒâ–</td></tr><tr><td>TrainEpochStats/avg_step_time</td><td>â–…â–‚â–â–†â–‚â–ˆâ–‚â–</td></tr><tr><td>TrainEpochStats/epoch_time</td><td>â–â–‚â–‚â–†â–„â–ˆâ–ƒâ–ƒ</td></tr><tr><td>TrainIterStats/current_lr</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>TrainIterStats/loader_time</td><td>â–„â–ˆâ–‡â–ƒâ–ˆâ–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚</td></tr><tr><td>TrainIterStats/loss</td><td>â–‡â–…â–ˆâ–†â–†â–„â–ƒâ–ƒâ–â–…â–ƒâ–‚â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚â–</td></tr><tr><td>TrainIterStats/loss_mel_ce</td><td>â–‡â–…â–ˆâ–†â–…â–„â–ƒâ–ƒâ–â–…â–ƒâ–‚â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚â–</td></tr><tr><td>TrainIterStats/loss_text_ce</td><td>â–‡â–„â–„â–†â–‡â–†â–‡â–…â–‚â–â–…â–…â–â–‡â–ˆâ–‚â–‚â–‚â–â–‡â–‡</td></tr><tr><td>TrainIterStats/step_time</td><td>â–„â–ˆâ–ˆâ–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–‚â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>EvalStats/avg_loader_time</td><td>0.01585</td></tr><tr><td>EvalStats/avg_loss</td><td>3.36816</td></tr><tr><td>EvalStats/avg_loss_mel_ce</td><td>3.34457</td></tr><tr><td>EvalStats/avg_loss_text_ce</td><td>0.02359</td></tr><tr><td>TrainEpochStats/avg_grad_norm</td><td>0</td></tr><tr><td>TrainEpochStats/avg_loader_time</td><td>0.01378</td></tr><tr><td>TrainEpochStats/avg_loss</td><td>0.04092</td></tr><tr><td>TrainEpochStats/avg_loss_mel_ce</td><td>3.41265</td></tr><tr><td>TrainEpochStats/avg_loss_text_ce</td><td>0.02454</td></tr><tr><td>TrainEpochStats/avg_step_time</td><td>3.15302</td></tr><tr><td>TrainEpochStats/epoch_time</td><td>2907.6965</td></tr><tr><td>TrainIterStats/current_lr</td><td>1e-05</td></tr><tr><td>TrainIterStats/loader_time</td><td>0.0148</td></tr><tr><td>TrainIterStats/loss</td><td>0.0395</td></tr><tr><td>TrainIterStats/loss_mel_ce</td><td>3.29025</td></tr><tr><td>TrainIterStats/loss_text_ce</td><td>0.02797</td></tr><tr><td>TrainIterStats/step_time</td><td>2.7167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xttsv2_finetune_20250504_1250</strong> at: <a href='https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune/runs/epaofufn' target=\"_blank\">https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune/runs/epaofufn</a><br> View project at: <a href='https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune' target=\"_blank\">https://wandb.ai/sliverwall-new-jersey-institute-of-technology/XTTS-v2%20Finetune</a><br>Synced 5 W&B file(s), 18 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250504_125022-epaofufn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''TRAINING: manual interupts will set model to output saves at given checkpoints'''\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb6645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
