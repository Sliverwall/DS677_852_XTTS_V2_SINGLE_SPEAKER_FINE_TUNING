{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4d3f78",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## Setup notes\n",
    "- If installing TTS package on a venv, install propper cuda enabled torch otherwise default torch will be installed, preventing cuda from being used.\n",
    "- Go to \"TTS\\tts\\layers\\tortoise\\arch_utils.py\" replace references of LogitWarper to LogitsProcessor\n",
    "- Go to \"TTS\\tts\\models\\xtts.py then to function get_compatible_checkpoint_state_dict. On line 714: checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"))[\"model\"]. Add the argument 'weights_only = False\": checkpoint = load_fsspec(model_path, map_location=torch.device(\"cpu\"), weights_only = False)[\"model\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2b8b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''Imports'''\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from trainer.logging.wandb_logger import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c75a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "12.4\n",
      "True\n",
      "NVIDIA GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "'''Display device used'''\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)           \n",
    "print(torch.cuda.is_available())    \n",
    "print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099c65f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set.\n"
     ]
    }
   ],
   "source": [
    "'''DOWNLOADS'''\n",
    "# Get XTTS files\n",
    "CHECKPOINT_PATH = './XTTS-files/'\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# DVAE files\n",
    "DVAE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(DVAE_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# DVAE download if not exists\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_LINK], CHECKPOINT_PATH, progress_bar=True)\n",
    "\n",
    "# XTTS v2.0 checkpoint\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# Transfer learning parameters. NOTE: Sets base model to use\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth\n",
    "\n",
    "# XTTS v2.0 download if not exists\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINT_PATH, progress_bar=True\n",
    "    )\n",
    "print(\"Paths set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08517a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DATA LOADING'''\n",
    "# Set lang\n",
    "LANGUAGE ='en'\n",
    "# Set to folder name that contains metadata.csv and wavs dir (with the .wav examples)\n",
    "DATASET= \"tom_hanks_dutch_house\"\n",
    "training_dir = f'./datasets/{DATASET}/' # change to folder w/ training examples\n",
    "\n",
    "# Dataset uses ljspeech format\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    meta_file_train=\"metadata.csv\", # metadata file w/ transcriptions\n",
    "    language=LANGUAGE,\n",
    "    path=training_dir\n",
    ")\n",
    "\n",
    "# Turn off eval split. Will evaluate manually\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_size=0.02 # Might change\n",
    ")\n",
    "\n",
    "\n",
    "'''MODIFY'''\n",
    "# Audio config\n",
    "audio_config = XttsAudioConfig(sample_rate=16000, dvae_sample_rate=16000, output_sample_rate=24000) \n",
    "\n",
    "# Speaker Reference: Match theses to the test sentences\n",
    "### Only need 1 speaker audio reference. Do not need to match voice to text\n",
    "SPEAKER_TEXT = [\n",
    "\"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "    \"This cake is great. It's so delicious and moist.\"\n",
    "]\n",
    "SPEAKER_REFERENCE = f\"datasets/{DATASET}/wavs/chunk_0009.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db444e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set Model arguments'''\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=143677, # Audio used for conditioning latents should be less than this \n",
    "    min_conditioning_length=66150, # 6\n",
    "    debug_loading_failures=True,\n",
    "    max_wav_length=255995, # Set >= longest audio in dataset  ~ 11.6.set to max 10\n",
    "    max_text_length=66150, # min 3\n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  \n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    gpt_num_audio_tokens=1026, \n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Set up configuration file'''\n",
    "'''TRAINING CONFIG'''\n",
    "OUT_PATH = './training_outputs/'\n",
    "\n",
    "RUN_NAME = f\"xttsv2_finetune_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "PROJECT_NAME = \"XTTS-v2 Finetune\"\n",
    "DASHBOARD_LOGGER = 'wandb'\n",
    "LOGGER_URI = None\n",
    "\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  \n",
    "\n",
    "BATCH_SIZE = 3 # 4 is common\n",
    "\n",
    "config = GPTTrainerConfig(\n",
    "    run_eval=True,\n",
    "    epochs = 2, # assuming you want to end training manually w/ keyboard interrupt\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    run_description=\"\"\"\n",
    "        GPT XTTS training\n",
    "        \"\"\",\n",
    "    dashboard_logger=DASHBOARD_LOGGER,\n",
    "    wandb_entity=None,\n",
    "    logger_uri=LOGGER_URI,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=0, # On Windows, num_loader_workers > 0 can break multiprocessing in PyTorch\n",
    "    eval_split_max_size=256, \n",
    "    print_step=50, \n",
    "    plot_step=100, \n",
    "    log_model_step=1000, \n",
    "    save_step=1000, # Needs to be an int\n",
    "    save_n_checkpoints=3, # Rotate last 3 checkpoints\n",
    "    save_checkpoints=True,\n",
    "    print_eval=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=5e-06,  \n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[ \n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[0],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE, \n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[1],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        }\n",
    "    ],\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eba573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Set up Trainer'''\n",
    "# Init model \n",
    "model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "# Init Trainer\n",
    "GRAD_ACUMM_STEPS = 84 # Note: GRAD_ACUMM_STEPS * BATCH_SIZE = 252\n",
    "START_WITH_EVAL = True  \n",
    "\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None, # Change to model path if resuming\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=START_WITH_EVAL,\n",
    "        grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe4fb20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''TRAINING: manual interupts will set model to output saves at given checkpoints'''\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb6645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
