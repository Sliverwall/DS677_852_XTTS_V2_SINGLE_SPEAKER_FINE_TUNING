{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb61b407",
   "metadata": {},
   "source": [
    "<center><h1> XTTS-v2: Single-Speaker Fine-Tuning </center></h1>\n",
    "<center> Roberto Caamano, Giuseppe Di Roberto </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32eed0b",
   "metadata": {},
   "source": [
    "# <img src=\"img/XTTS.png\" width=600 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "DATASET = \"noramlized_personal_voice\"\n",
    "\n",
    "\n",
    "BPE_path = \"XTTS-files/vocab.json\"\n",
    "\n",
    "checkpoint_dir = f\"training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\"\n",
    "config_path =f\"{model_path}/config.json\"\n",
    "\n",
    "speaker_ref = f\"datasets/{DATASET}/wavs/chunk_0016.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74133be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "text = \"Hello world, I can speak.\"\n",
    "\n",
    "# Init Xtts and load config object\n",
    "cfg = XttsConfig()\n",
    "cfg.load_json(config_path)\n",
    "\n",
    "\n",
    "# Init model and load from checkpoint\n",
    "model = Xtts.init_from_config(cfg)\n",
    "\n",
    "model.load_checkpoint(\n",
    "    cfg,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    vocab_path=vocab_path,\n",
    "    eval=True, \n",
    ")\n",
    "\n",
    "# Set to eval\n",
    "model.to(device).eval()\n",
    "\n",
    "# Get the gpt conditonal latent codes and speaker_encoder from the reference audio mel spectrogram\n",
    "gpt_cond_latent, speaker_encoder = model.get_conditioning_latents(\n",
    "    audio_path=[speaker_ref], # Speaker reference wav pointed here. Multiple can be used. Important for quality of output\n",
    "    gpt_cond_len=cfg.gpt_cond_len, #  Context window size of latents being passed to GPT \n",
    "    gpt_cond_chunk_len=cfg.gpt_cond_chunk_len, # How many chunks audio tokens split into before going to PercieverResampler\n",
    "    max_ref_length=cfg.max_ref_len, # Limits how much of the speaker reference audio is used.\n",
    ")\n",
    "\n",
    "\n",
    "# Model's inference method\n",
    "output = model.inference(\n",
    "            text=text, # Input text\n",
    "            language=\"en\", # Set language to english \n",
    "            gpt_cond_latent=gpt_cond_latent, # Pass conditional latents to GPT\n",
    "            speaker_embedding=speaker_encoder, # Pass Speaker Encoder to Decoder  \n",
    "            temperature=0.75,\n",
    "            speed=1,\n",
    "            length_penalty=cfg.length_penalty,\n",
    "            repetition_penalty=cfg.repetition_penalty,\n",
    "            top_k=cfg.top_k,\n",
    "            top_p=cfg.top_p,\n",
    "        )\n",
    "\n",
    "# Create wav tensor\n",
    "wav_tensor = torch.tensor(out[\"wav\"]).unsqueeze(0)  # shape: (1, samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Notes from TTS.tts.models.xtts\n",
    "\n",
    "gpt_codes = self.gpt.generate(\n",
    "                    cond_latents=gpt_cond_latent,\n",
    "                    text_inputs=text_tokens,\n",
    "                    input_tokens=None,\n",
    "                    do_sample=do_sample,\n",
    "                    top_p=top_p,\n",
    "                    top_k=top_k,\n",
    "                    temperature=temperature,\n",
    "                    num_return_sequences=self.gpt_batch_size,\n",
    "                    num_beams=num_beams,\n",
    "                    length_penalty=length_penalty,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                    output_attentions=False,\n",
    "                    **hf_generate_kwargs,\n",
    "                )\n",
    "\n",
    "\n",
    "gpt_latents = self.gpt(\n",
    "    text_tokens,\n",
    "    text_len,\n",
    "    gpt_codes,\n",
    "    expected_output_len,\n",
    "    cond_latents=gpt_cond_latent,\n",
    "    return_attentions=False,\n",
    "    return_latent=True,\n",
    ")\n",
    "                \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
