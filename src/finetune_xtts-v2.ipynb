{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3ae7ae-c0a3-4870-a6bd-42805b8e61a4",
   "metadata": {},
   "source": [
    "# Notebook Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376ed3b-6f43-43d1-af84-1ee76354d0f5",
   "metadata": {},
   "source": [
    "### Import necessary libraries for inference:\n",
    "- XTTS model configs and classes for TTS generation\n",
    "- PyTorch and torchaudio for tensor operations and audio I/O\n",
    "- OS and regex utilities for file handling and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b972be61-e87b-45bb-bcad-b71e3729f377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig\n",
    "from TTS.tts.models.xtts import XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "from trainer.logging.wandb_logger import WandbLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4d0a6-c261-4108-9670-786e5c3a5a1c",
   "metadata": {},
   "source": [
    "### Tests GPU availability\n",
    "This ensures that the model will use a GPU (if available) to accelerate inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cabbd0c-d293-4349-9bf8-5aa69eb02461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "12.6\n",
      "True\n",
      "NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "# Torch info\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.version.cuda)           \n",
    "print(torch.cuda.is_available())    \n",
    "print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173d9e5-8a15-4e9b-a7cc-4ff9e6c8ce64",
   "metadata": {},
   "source": [
    "# Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba90f0-6c6a-431e-9b97-983695d3bee2",
   "metadata": {},
   "source": [
    "This section ensures that all required XTTS model components are available locally. It checks for the presence of pretrained model files (for the DVAE, mel normalization, tokenizer, and the main XTTS checkpoint), and downloads them from Coqui's model hub if they are not already present. These files are used for model initialization and inference later. Model checkpoint files can also be found on [HuggingFace](https://huggingface.co/coqui/XTTS-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c9d621-ce29-453d-ba9f-867a831db2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory where model files will be stored\n",
    "CHECKPOINT_PATH = '<PATH>'\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# DVAE files\n",
    "DVAE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(DVAE_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# DVAE download if not exists\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\"Downloading DVAE files\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_LINK], CHECKPOINT_PATH, progress_bar=True)\n",
    "\n",
    "# URLs for XTTS v2.0 tokenizer and checkpoint\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# Transfer learning parameters - Sets base model to use\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth\n",
    "\n",
    "# XTTS v2.0 download if not exists\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\"Downloading XTTS v2.0 files\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINT_PATH, progress_bar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e15d7-54f0-45d1-aced-c36f9bdfcf89",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "This section prepares the dataset and configuration required for training and inference. It includes selecting a language, loading metadata and audio samples, specifying the reference speaker clip for voice cloning, and initializing model-specific arguments such as input length limits and file paths.\n",
    "\n",
    "LJSpeech is a commonly used dataset format for text-to-speech systems, consisting of a metadata.csv file where each row links a transcript to an audio clip. The dataset follows the structure: audio_filename|transcript|normalized_transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711f13db-e5bb-49f2-984e-0d5c3300fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set language for dataset\n",
    "LANGUAGE ='en'\n",
    "\n",
    "# Folder containing 'metadata.csv' and 'wavs/' directory with audio clips\n",
    "DATASET= \"<DATASET NAME>\"\n",
    "training_dir = f\"<DATASET PATH>/{DATASET}\"\n",
    "\n",
    "# Configure dataset format and paths\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    meta_file_train=\"metadata.csv\", # File containing transcription data\n",
    "    language=LANGUAGE,\n",
    "    path=training_dir\n",
    ")\n",
    "\n",
    "# Load training and evaluation samples (2% reserved for evaluation)\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=True,\n",
    "    eval_split_size=0.02,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415112c-4dc8-4839-8a08-cdc0d909a705",
   "metadata": {},
   "source": [
    "This sets up the audio configuration and speaker reference data. The speaker reference is used to guide the model in cloning or adapting to the target voice. It is very important that this matches the intended speaker for the generated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b169a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio config for model input/output and DVAE encoder\n",
    "audio_config = XttsAudioConfig(\n",
    "    sample_rate=22050, # Sample rate for internal processing\n",
    "    dvae_sample_rate=22050, # Sample rate for DVAE encoder\n",
    "    output_sample_rate=24000 # Final output audio sample rate\n",
    ")\n",
    "\n",
    "# Reference text samples used during evaluation\n",
    "SPEAKER_TEXT = [\n",
    "    \"Hello, I am not a real person but I have a real voice.\",\n",
    "    \"I love my new voice it sounds so good.\"\n",
    "]\n",
    "\n",
    "# Set reference audio clip path for speaker identity\n",
    "SPEAKER_REFERENCE = \"<PATH>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d985fe-7fb9-4b8a-b2ac-f78664da00c6",
   "metadata": {},
   "source": [
    "This defines model-specific arguments such as audio/text length limits, pretrained checkpoints, and architectural settings like token usage and audio token encoding. These values should reflect the XTTS-v2 model constraints and performance recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3de044-0dc5-4bc0-8a33-e33fbe9cdf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify model arguments\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=132300, # Maximum speaker reference length (~6 secs)\n",
    "    min_conditioning_length=66150, # Minimum speaker reference length (~3 secs)\n",
    "    debug_loading_failures=True, # Verbose debugging for audio/text loading fail\n",
    "    max_wav_length=255995, # Maximum sample audio duration (~11.6 seconds)\n",
    "    max_text_length=66150, # Maximum character length for text input\n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  \n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    gpt_num_audio_tokens=1026, # Max audio tokens allowed\n",
    "    gpt_start_audio_token=1024, # [START] token for audio in GPT\n",
    "    gpt_stop_audio_token=1025, # [STOP] token for audio in GPT\n",
    "    gpt_use_masking_gt_prompt_approach=True, # Enables ground-truth masking strategy\n",
    "    gpt_use_perceiver_resampler=True, # Use Perceiver Resampler for conditioning\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf99a01-5134-44c2-ace8-93eafe472a5f",
   "metadata": {},
   "source": [
    "# Training Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01060b27-3c5b-4997-9d6a-9e3110e9f121",
   "metadata": {},
   "source": [
    "This section sets up all necessary training parameters for fine-tuning XTTS-v2. It specifies output paths, batch sizes, evaluation settings, logging preferences, optimizer configuration, and training hyperparameters. The model is trained using the GPT-based XTTS trainer and supports integration with Weights & Biases (wandb) for experiment tracking.\n",
    "\n",
    "The training configuration is designed to handle smaller batch sizes using gradient accumulation (BATCH_SIZE * GRAD_ACCUM_STEPS = 252) to match Coqui’s recommendations. Additionally, test_sentences are provided to synthesize and log audio samples each epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "282032f8-aa01-4dab-9d0d-d3daab4d8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directory for saving checkpoints, logs, and training artifacts\n",
    "OUT_PATH = '<PATH>'\n",
    "if not os.path.exists(OUT_PATH):\n",
    "    os.makedirs(OUT_PATH)\n",
    "\n",
    "# Name of the run and project (used in logs and dashboard tracking)\n",
    "RUN_NAME = '<RUN NAME>'\n",
    "PROJECT_NAME = '<PROJECT NAME>'\n",
    "DASHBOARD_LOGGER = 'wandb' # Use Weights & Biases for logging\n",
    "LOGGER_URI = None\n",
    "\n",
    "# Batch size and gradient accumulation to meet recommendations\n",
    "OPTIMIZER_WD_ONLY_ON_WEIGHTS = True\n",
    "BATCH_SIZE = 3\n",
    "GRAD_ACUMM_STEPS = 84 \n",
    "START_WITH_EVAL = True # Begin training with an initial evaluation pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2642fe8-1811-4717-b31d-1a4b3b2038f7",
   "metadata": {},
   "source": [
    "This defines the full training configuration using GPTTrainerConfig. It includes all required runtime options such as the number of epochs, evaluation strategy, optimizer and learning rate schedule, checkpoint saving intervals, and test sentence setup. Adjustments can be made here to control training behavior or debug model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d01cfce-9c86-41a9-a0ee-33502dc92b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTTrainerConfig(\n",
    "    run_eval=True, # Whether to start with evaluation step\n",
    "    epochs = 40, # Total training epochs\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=PROJECT_NAME,\n",
    "    run_description=\"\"\"\n",
    "        GPT XTTS training\n",
    "        \"\"\",\n",
    "    dashboard_logger=DASHBOARD_LOGGER,\n",
    "    wandb_entity=None,\n",
    "    logger_uri=LOGGER_URI,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48, # Number of batches grouped internally for optimization\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=0,\n",
    "    eval_split_max_size=256, # Maximum number of samples taken from validation dataset for evaluation\n",
    "    print_step=50, # Number of steps between printing training statistics\n",
    "    plot_step=100, # Number of steps between plotting loss and metric graphs\n",
    "    log_model_step=1000, # Number of steps between logging model checkpoints for external tracking\n",
    "    save_step=1000, # Number of steps between saving model checkpoints locally\n",
    "    save_n_checkpoints=1, # Number of past checkpoints to keep (older deleted)\n",
    "    save_checkpoints=True, # Whether to save model checkpoints at regular intervals\n",
    "    print_eval=True, # Whether to print evaluation results during training\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=5e-06,  \n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[ \n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[0],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE, \n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": SPEAKER_TEXT[1],\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "    ],\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1dbccc-435c-4e05-bbaa-ecfd4429ed28",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9909675-27f6-4ba1-9040-a81dd3111c13",
   "metadata": {},
   "source": [
    "This section kicks off the training process. It begins by initializing the model using the configuration defined earlier, then wraps it in a Trainer class that handles batching, evaluation, and checkpointing. Training is started with .fit(), and it can manually interrupted (with Ctrl+C) to safely save progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13184e8-0f97-41a4-a1a4-426754b2e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XTTS model\n",
    "model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "# Set up trainer\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None, # Change to model path if resuming\n",
    "        skip_train_epoch=False, # Whether to skip training (eval/debug only)\n",
    "        start_with_eval=START_WITH_EVAL,\n",
    "        grad_accum_steps=GRAD_ACUMM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")\n",
    "\n",
    "# Start model training loop; safe to interrupt manually to trigger checkpoint save\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
