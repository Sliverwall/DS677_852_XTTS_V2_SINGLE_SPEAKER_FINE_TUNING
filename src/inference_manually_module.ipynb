{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a76157",
   "metadata": {},
   "source": [
    "# Inference_manually_module\n",
    "\n",
    "- https://docs.coqui.ai/en/latest/models/xtts.html\n",
    "- rename ~1G .pth to speaker_xtts.pth. This file is the speaker-embedding vector for the fine-tuned voice. XTTS uses this vector to adjust model to a specific voice.\n",
    "- rename one of the ~5.7G models to model.pth\n",
    "- No need to set paths directly to the model and speaker embeddings. Just set the dir. If the vocab.json is in the same dir, no need to use vocab_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea79abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5103d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genAudioManual(text: str,checkpoint_dir: str,vocab_path: str, reference_wav: str,\n",
    "                   output_path: str,\n",
    "                   split_sentences:bool=True,\n",
    "                   device: str = \"cuda:0\",temperature: float = 0.75,\n",
    ") -> str:\n",
    "    \n",
    "    ### Follow docs page for inference without the TTS wrapper.\n",
    "    \n",
    "    \n",
    "    # Load the config file in. \n",
    "    print(\"Loading model...\")\n",
    "    cfg = XttsConfig()\n",
    "    cfg.load_json(os.path.join(checkpoint_dir, \"config.json\"))\n",
    "\n",
    "    # Init model using the config. No TTS wrapper, do as done in the xtts_demo.py\n",
    "    model = Xtts.init_from_config(cfg)\n",
    "\n",
    "    # Load from checkpoint. Here is where the model gets loaded in using the base model/speaker embeedings learned\n",
    "    model.load_checkpoint(\n",
    "        cfg,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        vocab_path=vocab_path,\n",
    "        eval=True,\n",
    "        strict=False,\n",
    "        use_deepspeed=False, # Need Deepspeed for this. Difficult on Windows...\n",
    "    )\n",
    "\n",
    "    # Set to eval\n",
    "    model.to(device).eval()\n",
    "\n",
    "    #\n",
    "    print(\"Compute speaker latents...\")\n",
    "    \n",
    "    # This is from tortoise.py. Notes from original file:\n",
    "    '''\n",
    "    Transforms one or more voice_samples into a tuple (autoregressive_conditioning_latent, diffusion_conditioning_latent).\n",
    "    These are expressive learned latents that encode aspects of the provided clips like voice, intonation, and acoustic\n",
    "    properties.\n",
    "    '''\n",
    "    gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(\n",
    "        audio_path=[reference_wav],\n",
    "        gpt_cond_len=cfg.gpt_cond_len,\n",
    "        gpt_cond_chunk_len=cfg.gpt_cond_chunk_len,\n",
    "        max_ref_length=cfg.max_ref_len,\n",
    "    )\n",
    "    \n",
    "    if split_sentences:\n",
    "        # Break text into distinct sentences\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text.strip())\n",
    "    else:\n",
    "        sentences = [text]\n",
    "\n",
    "    segments = []\n",
    "    # Loop for through sentence. Do inference one at at time\n",
    "    for sentence in sentences:\n",
    "        print(f\"Generating audio for: {sentence}\")\n",
    "\n",
    "        out = model.inference(\n",
    "            text=sentence,\n",
    "            language=\"en\",\n",
    "            gpt_cond_latent=gpt_cond_latent,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "            temperature=temperature,\n",
    "            speed=1,\n",
    "            length_penalty=cfg.length_penalty,\n",
    "            repetition_penalty=cfg.repetition_penalty,\n",
    "            top_k=cfg.top_k,\n",
    "            top_p=cfg.top_p,\n",
    "        )\n",
    "        \n",
    "        # Create wav tensor then add to segements list\n",
    "        wav_tensor = torch.tensor(out[\"wav\"]).unsqueeze(0)  # shape: (1, samples)\n",
    "        segments.append(wav_tensor)\n",
    "\n",
    "\n",
    "    # Convert the output in wav format, set to a tensor so torchaudio can be used.\n",
    "    # Concatenate all wav tensors along the time axis (dim=1)\n",
    "    finalAudio = torch.cat(segments, dim=1)\n",
    "    \n",
    "    torchaudio.save(output_path, finalAudio, sample_rate=cfg.audio.output_sample_rate)\n",
    "    \n",
    "    print(f\"Output saved to {output_path}\")\n",
    "    # Return output path\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "931dc354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Compute speaker latents...\n",
      "Generating audio for: For our project together, we worked on fine tuning KoKey’s XTT S model on a single speaker.\n",
      "Generating audio for: XTT S is a multilingual Text to Speech model that is able to produce high quality synthetic speech.\n",
      "Generating audio for: Note that this voice was trained using audio samples of me reading.\n",
      "Generating audio for: So I may sound like I am reading directly from a script.\n",
      "I will first be walking us through the model shown here.\n",
      "Generating audio for: Beginning at the bottom of this flow chart, we can see the model’s inputs.\n",
      "Generating audio for: At the bottom, we can see three inputs.\n",
      "Generating audio for: A Reference spectrogram, a text input of some funny text, and a spectrogram marked as being the ground truth.\n",
      "Generating audio for: \n",
      "\n",
      "The spectrograms here are referring to mel spectrograms.\n",
      "Generating audio for: This is a specially formatted audio format that encodes for melody.\n",
      "Generating audio for: If you are into music, this may already be familiar to you.\n",
      "Generating audio for: Raw audio samples are often converted into mel spectrograms for text to speech models because it is a highly-compressed and information rich format.\n",
      "Generating audio for: Which is useful for the neural network.\n",
      "Generating audio for: \n",
      "\n",
      "The ground truth spectrogram here is used during training.\n",
      "Generating audio for: This is an audio sample from a sample batch that will be used as the target.\n",
      "Generating audio for: In cases of training the text input will be a transcript of the audio clip.\n",
      "Generating audio for: A training set is composed of a set of audio files, each file between 4 to 10 seconds, labeled with transcribed text.\n",
      "\n",
      "The reference spectrogram is used during both training and inference.\n",
      "Generating audio for: This audio clip also should be between 4 to 10 seconds, but is not labeled with a transcript.\n",
      "Generating audio for: This should be a high-quality audio sample of the target speaker.\n",
      "Generating audio for: This reference sample will be used by the model to extract information on the target voice.\n",
      "Generating audio for: For example if you are not fine-tuning the model, you can still get the model to mimic a voice using only a short reference audio sample.\n",
      "Generating audio for: It is important this sample is of high quality.\n",
      "Generating audio for: \n",
      "\n",
      "The text here during training is the labeled transcript for the target spectrogram.\n",
      "Generating audio for: When using the model for reference, this text is what the model audio will attempt to output.\n",
      "\n",
      "From this we can see the model’s general goal.\n",
      "Generating audio for: During training it will try to match the speech of the target spectrogram.\n",
      "Generating audio for: During inference it will try to convert the text into speech.\n",
      "Generating audio for: \n",
      "\n",
      "Moving up to the next layer, we have the processing units.\n",
      "Generating audio for: This VQ VAE is a type of discrete variational autoencoder.\n",
      "Generating audio for: It will use a learned codebook to match words in the target spectrogram into discrete words.\n",
      "Generating audio for: This turns a continuous task of having to match an infinite amount of possible words to a discrete problem, dramatically reducing the work the model needs to do.\n",
      "Generating audio for: The BPE is taking the text, then converting them into fixed subtokens.\n",
      "Generating audio for: So instead of needing unique tokens dedicated to every word and its varying tenses, it will take the base word, then modify it to match tense, or other grammatical transformations.\n",
      "\n",
      "The Perceiver conditioner is a combination of the Conditional Encoder and perciever resampler.\n",
      "[!] Warning: The text length exceeds the character limit of 250 for language 'en', this might cause truncated audio.\n",
      "Generating audio for: In combination, theses will take the reference audio clip, then encode for melody and rhythm in speech as well as speaker identity.\n",
      "Generating audio for: The conditional latent codes, alongside the audio code embeddings and text token embeddings from the target, will be fed to the main GPT unit.\n",
      "Generating audio for: Additionally, the speaker identity will separately be passed to the Decoder.\n",
      "Generating audio for: An interesting note here, it is the speaker encoder here that makes XTT S able to work with multiple kinds of voices without needing to fine-tune the GPT layer.\n",
      "Generating audio for: If training the same base model on multiple voices, you can fix the GPT layer and only save fine-tuned Speaker encoders which can be swapped in and out.\n",
      "Generating audio for: \n",
      "\n",
      "The GPT here will take the combined inputs then try to predict the next audio code token at each step.\n",
      "Generating audio for: The Duel language model heads here, one for text and one for audio, will be used to compute the loss.\n",
      "Generating audio for: The text head computes logits over the text vocabulary.\n",
      "Generating audio for: This is only done when training.\n",
      "Generating audio for: The audio head computes logits over the audio-codebook.\n",
      "Generating audio for: \n",
      "\n",
      "On the left side here, the Decoder will take the encoded audio output from the GPT, then reconstruct a mel spectrogram.\n",
      "Generating audio for: This is then passed to a gan unit.\n",
      "Generating audio for: The discriminator unit of the gan will judge the model’s output, this will be our gan loss.\n",
      "Generating audio for: This is done to try to improve the realism of the output.\n",
      "\n",
      "The speaker consistency loss is compares the final output to the reference spectrogram to improve consistency between them.\n",
      "Generating audio for: \n",
      "\n",
      "Moving down to some code, here is a quick walk through of the inputs moving along during inference.\n",
      "Generating audio for: First we need to set file path to the needed units.\n",
      "Generating audio for: This includes where our vocab list is, our model and configuration, and the speaker reference audio file.\n",
      "\n",
      "Then we can define a text input, initialize and load our model, then set the mode to evaluation mode.\n",
      "Generating audio for: To process the inputs, we can call the model’s get conditioning latents method.\n",
      "Generating audio for: This will take as input the speaker reference, then output the conditional latent codes and speaker encoder.\n",
      "\n",
      "We can then pass these units over to the model’s inference method.\n",
      "Generating audio for: This will output a dictionary which contains a waveform array.\n",
      "Generating audio for: To save this output, simply convert it to a tensor and use torchaudio.\n",
      "Generating audio for: Notice many hyperparameters here are set based on the configuration file.\n",
      "Generating audio for: This will be explained later during the fine-tuning section.\n",
      "\n",
      "\n",
      "Now that we have a basic overview of the model’s architecture, we can look at the data preparation process.\n",
      "Generating audio for: For fine-tuning each speaker, we began with an audio file in waveform format.\n",
      "Generating audio for: This raw audio file should at least be an hour long, and contain mostly audio from the target speaker.\n",
      "Generating audio for: Since this will be used for training, it is important that external noises, like music or other non speaker audio, not be included in this file.\n",
      "Generating audio for: We observed that leaving theses noises in will cause them to be reproduced during inference.\n",
      "Generating audio for: For example, you may hear me taking the occasional deep breath, since I did not edit theses out of my training set.\n",
      "Generating audio for: \n",
      "\n",
      "XTT S does not take long audio files.\n",
      "Generating audio for: The max and minimum lengths can be set in the fine-tuning configuration, but we found it is best to keep each sample between four and eleven  seconds.\n",
      "Generating audio for: There are various possible methods that can be used to chunk the raw audio file into many different samples.\n",
      "Generating audio for: The method I used was to sample audio lengths, bound between four and eleven seconds, from a normal distribution.\n",
      "Generating audio for: This ensures that my audio lengths are diverse, and that the typical six second length is well represented.\n",
      "Generating audio for: A similar method would be to use a uniform distribution, ensuring each possible input length is shown equally.\n",
      "Generating audio for: Yet another method we observed that performs well is to chunk the audio such that each sample represents one full sentence.\n",
      "\n",
      "While chunking the audio files, it is important to give the output file names descriptive names.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating audio for: When forming the final dataset, we will need to associate a transcript with each chunk.\n",
      "Generating audio for: For this, we used a file naming system that adds the iteration number for the chunk in the file name.\n",
      "Generating audio for: This was possible because we chunked the auto files sequentially.\n",
      "Generating audio for: \n",
      "\n",
      "Once chunking is complete, you should inspect the chunks to ensure no obvious audio errors are present.\n",
      "Generating audio for: For example, it is possible if you did not chunk using the sentence method, that a chunk may contain no speech.\n",
      "Generating audio for: Remove chunks with obvious errors or poor quality.\n",
      "Generating audio for: Since this is a tedious task, and may not be feasible if handling hours worth of audio, later I will discuss other methods for removing poor quality chunks.\n",
      "\n",
      "Next in the data processing workflow, we need to associate text labels for each chunk.\n",
      "Generating audio for: The format of this dataset file will be a text file with three features.\n",
      "Generating audio for: Feature one is the file name for the chunk.\n",
      "Generating audio for: Feature two is the transcription text for the audio content.\n",
      "Generating audio for: Feature three is a normalized transcription.\n",
      "Generating audio for: This means that, for example, if the transcription had the number fourteen hundred, you could write it as one thousand four hundred in the normalized column.\n",
      "Generating audio for: It is important to note that the file structure for this text file uses the vertical bar symbol as a separator.\n",
      "Generating audio for: \n",
      "\n",
      "For the transcription process, we used Open AI’s Whisper model.\n",
      "Generating audio for: This allowed us to transcribe hours of audio without having to manually go through each chunk.\n",
      "Generating audio for: However, this process is not perfect.\n",
      "Generating audio for: There are sometimes transcription errors, and this can cause errors.\n",
      "Generating audio for: If you use this method, you can scan the transcription file manually, and check for any obvious error.\n",
      "Generating audio for: This is an iterative process and the more work you put in here, the better the model will sound.\n",
      "\n",
      "One optional method we used to clean up transcription and other chunk errors was outlier detection.\n",
      "Generating audio for: For example, using the transcribed text and audio files, we used Z-score outlier detection to remove chunks that had unusually long or short words to seconds metrics.\n",
      "Generating audio for: This squashes the variance in word to audio length duration within the dataset, hopefully giving the model more consistent samples without reducing audio length duration.\n",
      "Generating audio for: Then finally, once the data file is prepared with chunked audio files and transcriptions, we are ready to move onto fine-tuning the model.\n",
      "Generating audio for: \n",
      "\n",
      "Note that there are many other options here that can be used to improve the final output.\n",
      "Generating audio for: For example, the outlier detection section can be expanded to capture more nuanced deviations.\n",
      "Generating audio for: The raw audio file can have its sample rate normalized to be more consistent.\n",
      "Generating audio for: \n",
      "\n",
      "An important observation that should be noted is the context for the speaker.\n",
      "Generating audio for: For the models trained in this project, we used samples from speakers reading.\n",
      "Generating audio for: People tend to have a specific tone and cadence when reading, which really affects the model output.\n",
      "Generating audio for: If you want normal and casual speech, do your best to get samples from people speaking casually.\n",
      "Output saved to output/project_voice_test.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "vocab_path = \"XTTS-files/vocab.json\"\n",
    "\n",
    "# models = [\"xttsv2_finetune_20250418_2027-April-18-2025_08+27PM-7d4c6a1\", \n",
    "#          \"xttsv2_finetune_20250430_2033-April-30-2025_08+33PM-ca1939c\",\n",
    "#          \"xttsv2_finetune_20250503_2111-May-03-2025_09+11PM-ca1939c\"]\n",
    "\n",
    "models = [\"xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\"]\n",
    "\n",
    "for i, voice in enumerate(models):\n",
    "    checkpoint_dir = f\"training_outputs/{voice}\"\n",
    "\n",
    "\n",
    "    DATASET = \"noramlized_personal_voice\"\n",
    "    speaker_ref = f\"datasets/{DATASET}/wavs/chunk_0016.wav\"\n",
    "\n",
    "    text = '''\n",
    "    \n",
    "For our project together, we worked on fine tuning KoKey’s XTT S model on a single speaker. XTT S is a multilingual Text to Speech model that is able to produce high quality synthetic speech. Note that this voice was trained using audio samples of me reading. So I may sound like I am reading directly from a script.\n",
    "I will first be walking us through the model shown here. Beginning at the bottom of this flow chart, we can see the model’s inputs. At the bottom, we can see three inputs. A Reference spectrogram, a text input of some funny text, and a spectrogram marked as being the ground truth. \n",
    "\n",
    "The spectrograms here are referring to mel spectrograms. This is a specially formatted audio format that encodes for melody. If you are into music, this may already be familiar to you. Raw audio samples are often converted into mel spectrograms for text to speech models because it is a highly-compressed and information rich format. Which is useful for the neural network. \n",
    "\n",
    "The ground truth spectrogram here is used during training. This is an audio sample from a sample batch that will be used as the target. In cases of training the text input will be a transcript of the audio clip. A training set is composed of a set of audio files, each file between 4 to 10 seconds, labeled with transcribed text.\n",
    "\n",
    "The reference spectrogram is used during both training and inference. This audio clip also should be between 4 to 10 seconds, but is not labeled with a transcript. This should be a high-quality audio sample of the target speaker. This reference sample will be used by the model to extract information on the target voice. For example if you are not fine-tuning the model, you can still get the model to mimic a voice using only a short reference audio sample. It is important this sample is of high quality. \n",
    "\n",
    "The text here during training is the labeled transcript for the target spectrogram. When using the model for reference, this text is what the model audio will attempt to output.\n",
    "\n",
    "From this we can see the model’s general goal. During training it will try to match the speech of the target spectrogram. During inference it will try to convert the text into speech. \n",
    "\n",
    "Moving up to the next layer, we have the processing units. This VQ VAE is a type of discrete variational autoencoder. It will use a learned codebook to match words in the target spectrogram into discrete words. This turns a continuous task of having to match an infinite amount of possible words to a discrete problem, dramatically reducing the work the model needs to do. The BPE is taking the text, then converting them into fixed subtokens. So instead of needing unique tokens dedicated to every word and its varying tenses, it will take the base word, then modify it to match tense, or other grammatical transformations.\n",
    "\n",
    "The Perceiver conditioner is a combination of the Conditional Encoder and perciever resampler. In combination, theses will take the reference audio clip, then encode for melody and rhythm in speech as well as speaker identity. The conditional latent codes, alongside the audio code embeddings and text token embeddings from the target, will be fed to the main GPT unit. Additionally, the speaker identity will separately be passed to the Decoder. An interesting note here, it is the speaker encoder here that makes XTT S able to work with multiple kinds of voices without needing to fine-tune the GPT layer. If training the same base model on multiple voices, you can fix the GPT layer and only save fine-tuned Speaker encoders which can be swapped in and out. \n",
    "\n",
    "The GPT here will take the combined inputs then try to predict the next audio code token at each step. The Duel language model heads here, one for text and one for audio, will be used to compute the loss. The text head computes logits over the text vocabulary. This is only done when training. The audio head computes logits over the audio-codebook. \n",
    "\n",
    "On the left side here, the Decoder will take the encoded audio output from the GPT, then reconstruct a mel spectrogram. This is then passed to a gan unit. The discriminator unit of the gan will judge the model’s output, this will be our gan loss. This is done to try to improve the realism of the output.\n",
    "\n",
    "The speaker consistency loss is compares the final output to the reference spectrogram to improve consistency between them. \n",
    "\n",
    "Moving down to some code, here is a quick walk through of the inputs moving along during inference. First we need to set file path to the needed units. This includes where our vocab list is, our model and configuration, and the speaker reference audio file.\n",
    "\n",
    "Then we can define a text input, initialize and load our model, then set the mode to evaluation mode. To process the inputs, we can call the model’s get conditioning latents method. This will take as input the speaker reference, then output the conditional latent codes and speaker encoder.\n",
    "\n",
    "We can then pass these units over to the model’s inference method. This will output a dictionary which contains a waveform array. To save this output, simply convert it to a tensor and use torchaudio. Notice many hyperparameters here are set based on the configuration file. This will be explained later during the fine-tuning section.\n",
    "\n",
    "\n",
    "Now that we have a basic overview of the model’s architecture, we can look at the data preparation process. For fine-tuning each speaker, we began with an audio file in waveform format. This raw audio file should at least be an hour long, and contain mostly audio from the target speaker. Since this will be used for training, it is important that external noises, like music or other non speaker audio, not be included in this file. We observed that leaving theses noises in will cause them to be reproduced during inference. For example, you may hear me taking the occasional deep breath, since I did not edit theses out of my training set. \n",
    "\n",
    "XTT S does not take long audio files. The max and minimum lengths can be set in the fine-tuning configuration, but we found it is best to keep each sample between four and eleven  seconds. There are various possible methods that can be used to chunk the raw audio file into many different samples. The method I used was to sample audio lengths, bound between four and eleven seconds, from a normal distribution. This ensures that my audio lengths are diverse, and that the typical six second length is well represented. A similar method would be to use a uniform distribution, ensuring each possible input length is shown equally. Yet another method we observed that performs well is to chunk the audio such that each sample represents one full sentence.\n",
    "\n",
    "While chunking the audio files, it is important to give the output file names descriptive names. When forming the final dataset, we will need to associate a transcript with each chunk. For this, we used a file naming system that adds the iteration number for the chunk in the file name. This was possible because we chunked the auto files sequentially. \n",
    "\n",
    "Once chunking is complete, you should inspect the chunks to ensure no obvious audio errors are present. For example, it is possible if you did not chunk using the sentence method, that a chunk may contain no speech. Remove chunks with obvious errors or poor quality. Since this is a tedious task, and may not be feasible if handling hours worth of audio, later I will discuss other methods for removing poor quality chunks.\n",
    "\n",
    "Next in the data processing workflow, we need to associate text labels for each chunk. The format of this dataset file will be a text file with three features. Feature one is the file name for the chunk. Feature two is the transcription text for the audio content. Feature three is a normalized transcription. This means that, for example, if the transcription had the number fourteen hundred, you could write it as one thousand four hundred in the normalized column. It is important to note that the file structure for this text file uses the vertical bar symbol as a separator. \n",
    "\n",
    "For the transcription process, we used Open AI’s Whisper model. This allowed us to transcribe hours of audio without having to manually go through each chunk. However, this process is not perfect. There are sometimes transcription errors, and this can cause errors. If you use this method, you can scan the transcription file manually, and check for any obvious error. This is an iterative process and the more work you put in here, the better the model will sound.\n",
    "\n",
    "One optional method we used to clean up transcription and other chunk errors was outlier detection. For example, using the transcribed text and audio files, we used Z-score outlier detection to remove chunks that had unusually long or short words to seconds metrics. This squashes the variance in word to audio length duration within the dataset, hopefully giving the model more consistent samples without reducing audio length duration. Then finally, once the data file is prepared with chunked audio files and transcriptions, we are ready to move onto fine-tuning the model. \n",
    "\n",
    "Note that there are many other options here that can be used to improve the final output. For example, the outlier detection section can be expanded to capture more nuanced deviations. The raw audio file can have its sample rate normalized to be more consistent. \n",
    "\n",
    "An important observation that should be noted is the context for the speaker. For the models trained in this project, we used samples from speakers reading. People tend to have a specific tone and cadence when reading, which really affects the model output. If you want normal and casual speech, do your best to get samples from people speaking casually. \n",
    "    '''\n",
    "\n",
    "    # Example call:\n",
    "    out = genAudioManual(\n",
    "        text=text,\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        vocab_path=vocab_path,\n",
    "        reference_wav=speaker_ref,\n",
    "        output_path=f\"output/project_voice_test.wav\",\n",
    "        split_sentences=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9283cae",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "split work:\n",
    "\n",
    "--model arch\n",
    "--Report model arch intro, BDE, dVAE, Conditioning encoder, the perceiver resampler.\n",
    "Introduction section I\n",
    "--VI inference\n",
    "--V. Fine-tuning: Section B: Training and C. Experiments.\n",
    "-- Mention in training section loss metrics were not used to determine best model. Done by sampling checkpoints by ear.\n",
    "VII Novel Future Applications\n",
    "Acknowledgements\n",
    "\n",
    "\n",
    "Presentation:\n",
    "1. Quick intro\n",
    "2. Model overview\n",
    "3. Data prep\n",
    "4. Fine tuning\n",
    "\n",
    "notes: Use jupyter notebooks. They should contain markdown blocks before each section clearing explaining things"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
