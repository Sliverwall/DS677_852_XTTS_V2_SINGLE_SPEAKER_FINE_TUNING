{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb0a0c0",
   "metadata": {},
   "source": [
    "# Transcribe_audio notebook\n",
    "\n",
    "## Purpose\n",
    "---\n",
    "- Uses openai's whisper-large-v3 model to take sample audio files, then automate the transcription process.\n",
    "- Coqui-Ai XTTS fine-tuning process requires a text-transcription for each audio file. If an audio sample does not have this, it would be difficult to write, by hand, the text needed.\n",
    "- Will be used when annotating speech from personal audio samples as well.\n",
    "---\n",
    "\n",
    "## How to use\n",
    "---\n",
    "- Requires torch and HuggingFace's transformers API to use the whisper-large-v3 model.\n",
    "- Define an import dir path where all your .wav audio files exist. \n",
    "- Define an output path for a csv file. Here, as each audio file is transcribed, its file-name and transcription will be written to the output CSV. This can be used as the metadata file for the fine-tuning process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3e32938",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Requires FFMEG to be installed for whipser model'''\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a747a238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "'''Load in whipster model using transformers api'''\n",
    "# Set device and torch data type\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "print(device)\n",
    "\n",
    "# Model identifier\n",
    "model_id = \"openai/whisper-large-v3\" # Was about 3G\n",
    "\n",
    "# Load the model and move it to the selected device\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=False, \n",
    "    use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Load the processor\n",
    "processor = AutoProcessor.from_pretrained(model_id, language='en')\n",
    "\n",
    "# Add a new special pad token (string) to the tokenizer\n",
    "if processor.tokenizer.pad_token_id == processor.tokenizer.eos_token_id:\n",
    "    processor.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    processor.tokenizer.pad_token_id = processor.tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "\n",
    "# Create the speech recognition pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    batch_size=32, \n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0c9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Step up paths for imput and output files'''\n",
    "# Define a path to an output CSV to save transcriptions\n",
    "outputPath = \"datasets/noramlized_personal_voice/metadata.csv\"\n",
    "\n",
    "# Define where sample audio files are coming from\n",
    "audioDir = \"datasets/noramlized_personal_voice/wavs/\"\n",
    "\n",
    "# Read in all files from chosen dir\n",
    "fileList = os.listdir(audioDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2055cda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing chunk_0000.wav...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing chunk_0025.wav...\n",
      "Transcribing chunk_0050.wav...\n",
      "Transcribing chunk_0075.wav...\n",
      "Transcribing chunk_0100.wav...\n",
      "Transcribing chunk_0125.wav...\n",
      "Transcribing chunk_0150.wav...\n",
      "Transcribing chunk_0175.wav...\n",
      "Transcribing chunk_0200.wav...\n",
      "Transcribing chunk_0225.wav...\n",
      "Transcribing chunk_0250.wav...\n",
      "Transcribing chunk_0275.wav...\n",
      "Transcribing chunk_0300.wav...\n",
      "Transcribing chunk_0325.wav...\n",
      "Transcribing chunk_0350.wav...\n",
      "Transcribing chunk_0375.wav...\n",
      "Transcribing chunk_0400.wav...\n",
      "Transcribing chunk_0425.wav...\n",
      "Transcribing chunk_0450.wav...\n",
      "Transcribing chunk_0475.wav...\n",
      "Transcribing chunk_0500.wav...\n",
      "Transcribing chunk_0525.wav...\n",
      "Transcribing chunk_0550.wav...\n",
      "Transcribing chunk_0575.wav...\n",
      "Transcribing chunk_0600.wav...\n",
      "Transcribing chunk_0625.wav...\n",
      "Transcribing chunk_0650.wav...\n",
      "Transcribing chunk_0675.wav...\n",
      "Transcribing chunk_0700.wav...\n"
     ]
    }
   ],
   "source": [
    "'''Transcribe sample files'''\n",
    "# Init list to hold all samples \n",
    "samples = []\n",
    "\n",
    "'''Loop here to go through multiple .wav files if needed'''\n",
    "for i in range(len(fileList)): \n",
    "    # Specify the path to your local .wav file\n",
    "    fileName = fileList[i]\n",
    "    audioPath = audioDir + fileName\n",
    "    # Msg to show transcription is proceeding\n",
    "    if i % 25 == 0:\n",
    "        print(f\"Transcribing {fileName}...\")\n",
    "    # Run the pipeline on the .wav file\n",
    "    result = pipe(audioPath)[\"text\"]\n",
    "    # LJ speech format (filename, transcript, normalised transcript)\n",
    "    samples.append((fileName.split('.')[0], result, result)) # no need to normalzied when fine-tuning. Just duplicate 2nd col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bffb2a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcriptions written to: datasets/noramlized_personal_voice/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# Write the samples list to output csv\n",
    "with open(outputPath, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "    # create csv writer\n",
    "    csvWriter = csv.writer(f, delimiter='|')\n",
    "    \n",
    "    # Note: No need for headers in LJ sppech format...\n",
    "    \n",
    "    # Write each sample to the CSV file\n",
    "    for entry in samples:\n",
    "        csvWriter.writerow(entry)\n",
    "\n",
    "print(\"Transcriptions written to:\", outputPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
