{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c5547c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and concatenating input files.\n",
      "Combined loudness before normalization: -30.58 dBFS\n",
      "Normalized loudness to -20.0 dBFS\n",
      "Audio written to: audio\\noramlized_personal_voice.wav\n",
      "Resampling audio to 24kHz.\n",
      "Original audio: 22050 Hz, 2 channel(s), 16-bit\n",
      "Resampling to 22050 Hz, mono, 16-bit PCM.\n",
      "Resampling complete and file overwritten.\n",
      "Audio resampling complete.\n",
      "Chunking by sentence using WhisperX.\n",
      "Using device: cuda\n",
      "Loading model and params...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\12017\\.cache\\huggingface\\hub\\models--Systran--faster-whisper-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\inspect.py:869: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      ">>Performing voice activity detection using Pyannote...\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12017\\Desktop\\NJIT\\DS677_852_Project\\src\\venv\\lib\\site-packages\\pyannote\\audio\\utils\\reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n",
      "It can be re-enabled by calling\n",
      "   >>> import torch\n",
      "   >>> torch.backends.cuda.matmul.allow_tf32 = True\n",
      "   >>> torch.backends.cudnn.allow_tf32 = True\n",
      "See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import argparse\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "from scipy.stats import truncnorm, zscore\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Faster inference with TF32\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# Sentence level transcription\n",
    "import whisperx\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Define upper & lower bounds for audio in seconds\n",
    "MIN_SEC, MAX_SEC = 3, 11.6\n",
    "\n",
    "# Define WhisperX parameters\n",
    "BATCH_SIZE = 16\n",
    "COMPUTE_TYPE = \"float16\"\n",
    "WHISPER_MODEL = \"small\" # Options: tiny, base, small, medium, large-v2, large-v3\n",
    "\n",
    "def yt_download(url):\n",
    "    \"\"\"\n",
    "    Downloads a wav file from given YouTube url.\n",
    "    \"\"\"\n",
    "    # Create audio folder\n",
    "    audio_dir = f\"./audio/\"\n",
    "    if not os.path.exists(audio_dir):\n",
    "        os.makedirs(audio_dir)\n",
    "\n",
    "    # Get video title (used later to create dir)\n",
    "    result = subprocess.run(\n",
    "        [\"yt-dlp\", \"--get-title\", url],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    title = result.stdout.strip()\n",
    "\n",
    "    # Clean title by removing special characters\n",
    "    title = re.sub(r'[^a-zA-Z0-9_\\- ]', '', title)\n",
    "\n",
    "    yt_audio_path = os.path.join(audio_dir, f\"{title}.wav\")\n",
    "\n",
    "    subprocess.run([\n",
    "        \"yt-dlp\",\n",
    "        \"-f\", \"bestaudio\",\n",
    "        \"--extract-audio\",\n",
    "        \"--audio-format\", \"wav\", # convert to wav as this is XTTS prefered format\n",
    "        \"-o\", yt_audio_path,\n",
    "        url\n",
    "    ])\n",
    "\n",
    "    return yt_audio_path, title\n",
    "\n",
    "def load_and_concat(file_paths, title, normalize_dbfs=-20.0):\n",
    "    # Create audio folder\n",
    "    audio_dir = f\"./audio/\"\n",
    "    if not os.path.exists(audio_dir):\n",
    "        os.makedirs(audio_dir)\n",
    "    \n",
    "    output_path = os.path.join(\"audio\", f\"{title}.wav\")\n",
    "\n",
    "    combined = AudioSegment.from_file(file_paths)\n",
    "\n",
    "    # Normalize loudness\n",
    "    print(f\"Combined loudness before normalization: {combined.dBFS:.2f} dBFS\")\n",
    "    change_dBFS = normalize_dbfs - combined.dBFS\n",
    "    combined = combined.apply_gain(change_dBFS)\n",
    "    print(f\"Normalized loudness to {normalize_dbfs} dBFS\")\n",
    "\n",
    "    combined.export(output_path, format=\"wav\")\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def resample_audio(audio_path):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "\n",
    "    frame_rate = audio.frame_rate\n",
    "    channels = audio.channels\n",
    "    sample_width_bits = audio.sample_width * 8\n",
    "\n",
    "    print(f\"Original audio: {frame_rate} Hz, {channels} channel(s), {sample_width_bits}-bit\")\n",
    "\n",
    "    if frame_rate != 22050 or channels != 1 or sample_width_bits != 16:\n",
    "        print(\"Resampling to 22050 Hz, mono, 16-bit PCM.\")\n",
    "        resampled = audio.set_frame_rate(22050).set_channels(1).set_sample_width(2)\n",
    "        resampled.export(audio_path, format=\"wav\")\n",
    "        print(\"Resampling complete and file overwritten.\")\n",
    "    else:\n",
    "        print(\"Audio already meets the required format.\")\n",
    "\n",
    "    return audio_path\n",
    "\n",
    "def chunk_sentences(\n",
    "    audio_path,\n",
    "    title,\n",
    "    min_sec: float = MIN_SEC,\n",
    "    max_sec: float = MAX_SEC,\n",
    "    char_limit: int = 250,\n",
    "    compute_type: str = COMPUTE_TYPE,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    whisper_model: str = WHISPER_MODEL,\n",
    "):\n",
    "    dataset_dir = os.path.join(\"./datasets\", title)\n",
    "    wavs_dir     = os.path.join(dataset_dir, \"wavs\")\n",
    "    os.makedirs(wavs_dir, exist_ok=True)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # WhisperX transcription + alignment\n",
    "    print(\"Loading model and params...\")\n",
    "    model      = whisperx.load_model(whisper_model, device=device, compute_type=compute_type)\n",
    "    audio_arr  = whisperx.load_audio(audio_path)\n",
    "    base       = model.transcribe(audio_arr, batch_size=batch_size, language=\"en\")\n",
    "    align_mdl, meta = whisperx.load_align_model(\"en\", device)\n",
    "    aligned    = whisperx.align(base[\"segments\"], align_mdl, meta, audio_arr,\n",
    "                                device, return_char_alignments=False)\n",
    "\n",
    "    word_segs  = aligned[\"word_segments\"]\n",
    "    full_text  = \" \".join(w[\"word\"] for w in word_segs)\n",
    "    sentences  = sent_tokenize(full_text)\n",
    "\n",
    "    # map sentences -> (start, end)\n",
    "    print(\"Mapping chunks\")\n",
    "    sent_segs, idx = [], 0\n",
    "    for sent in sentences:\n",
    "        n = len(sent.split())\n",
    "        if idx + n > len(word_segs):\n",
    "            break\n",
    "        sent_segs.append({\n",
    "            \"text\":  sent,\n",
    "            \"start\": word_segs[idx][\"start\"],\n",
    "            \"end\":   word_segs[idx + n - 1][\"end\"],\n",
    "        })\n",
    "        idx += n\n",
    "\n",
    "    # Build chunks respecting all limits\n",
    "    print(\"Building chunks\")\n",
    "    chunks, buf, dur, txt = [], [], 0.0, \"\"\n",
    "    for seg in sent_segs:\n",
    "        seg_dur  = seg[\"end\"] - seg[\"start\"]\n",
    "        seg_text = seg[\"text\"]\n",
    "        # Case 1: sentence is valid and fits limits\n",
    "        if min_sec <= seg_dur <= max_sec and len(seg_text) <= char_limit:\n",
    "            if buf:\n",
    "                chunks.append(buf)\n",
    "                buf, dur, txt = [], 0.0, \"\"\n",
    "            chunks.append([seg])\n",
    "            continue\n",
    "\n",
    "        # Case 2: add sentence to buffer\n",
    "        if dur + seg_dur > max_sec or len(txt) + 1 + len(seg_text) > char_limit:\n",
    "            if dur >= min_sec:\n",
    "                chunks.append(buf)\n",
    "                buf, dur, txt = [], 0.0, \"\"\n",
    "            else:\n",
    "                # buffer too short\n",
    "                pass\n",
    "\n",
    "        buf.append(seg)\n",
    "        dur += seg_dur\n",
    "        txt += (\" \" if txt else \"\") + seg_text\n",
    "\n",
    "    if buf:\n",
    "        chunks.append(buf)\n",
    "\n",
    "    # Write wavs + metadata\n",
    "    audio               = AudioSegment.from_file(audio_path)\n",
    "    metadata_path       = os.path.join(dataset_dir, \"metadata.csv\")\n",
    "    with open(metadata_path, \"w\", encoding=\"utf‑8‑sig\") as meta_f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            start  = int(chunk[0][\"start\"] * 1000)\n",
    "            end    = int(chunk[-1][\"end\"] * 1000)\n",
    "            if end <= start:\n",
    "                continue\n",
    "            piece  = audio[start:end]\n",
    "            if len(piece) < min_sec*1000 or len(piece) > max_sec*1000:\n",
    "                continue # double‑check duration\n",
    "            fname  = f\"chunk_{i:04}.wav\"\n",
    "            piece.export(os.path.join(wavs_dir, fname), format=\"wav\")\n",
    "\n",
    "            text   = \" \".join(c[\"text\"] for c in chunk)\n",
    "            if len(text) > char_limit: # final text safety check\n",
    "                # Truncate at nearest word before limit\n",
    "                text = text[:char_limit].rsplit(\" \", 1)[0] + \" …\"\n",
    "\n",
    "            # Write LJSpeech‑style row: <id>|<text>|<text>\n",
    "            meta_f.write(f\"{os.path.splitext(fname)[0]}|{text}|{text}\\n\")\n",
    "\n",
    "    return dataset_dir, wavs_dir\n",
    "\n",
    "def main():\n",
    "    title = \"noramlized_personal_voice\"\n",
    "    audio_input = \"./audio/set_1_123_min.wav\"\n",
    "    \n",
    "    url = None\n",
    "    \n",
    "    if url:\n",
    "        print('Downloading YouTube video...')\n",
    "        audio_path, title = yt_download(url)\n",
    "        print(f'YouTube video: {title} downloaded.')\n",
    "    else:\n",
    "        print('Loading and concatenating input files.')\n",
    "        audio_path = load_and_concat(audio_input, title)\n",
    "        print(f\"Audio written to: {audio_path}\")\n",
    "\n",
    "        print('Resampling audio to 24kHz.')\n",
    "        audio_path = resample_audio(audio_path)\n",
    "        print('Audio resampling complete.')\n",
    "\n",
    "        print(\"Chunking by sentence using WhisperX.\")\n",
    "        dataset_dir, wavs_dir = chunk_sentences(audio_path, title)\n",
    "        print(f\"Chunks and metadata saved in {dataset_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
