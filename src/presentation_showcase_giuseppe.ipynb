{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b8c922",
   "metadata": {},
   "source": [
    "<center><h1> XTTS-v2: Single-Speaker Fine-Tuning </center></h1>\n",
    "<center> Roberto Caamano, Giuseppe Di Roberto </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1a9b8",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)  \n",
    "2. [Model Architecture Overview](#model-architecture-overview)  \n",
    "3. [Data Preparation Workflow](#data-preparation-workflow)  \n",
    "4. [Fine-Tuning Process](#fine-tuning-process)  \n",
    "5. [Live Demo](#live-demo)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545f37f",
   "metadata": {},
   "source": [
    "<center><h1> Introduction </center></h1>\n",
    "\n",
    "## Note we will be using syntheic voices for this presentation. Theses voices come from models fine-tuned by us, on our own personal voices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa098db",
   "metadata": {},
   "source": [
    "<center><h1> XTTS Model Architecture Overview </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a52de4",
   "metadata": {},
   "source": [
    "# <img src=\"img/XTTS.png\" width=600 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up file paths\n",
    "\n",
    "BPE_path = \"XTTS-files/vocab.json\"\n",
    "\n",
    "checkpoint_dir = f\"training_outputs/xttsv2_finetune_20250504_1250-May-04-2025_12+50PM-ca1939c\"\n",
    "config_path =f\"{model_path}/config.json\"\n",
    "\n",
    "speaker_ref = f\"datasets/noramlized_personal_voice/wavs/chunk_0016.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input text\n",
    "text = \"Hello world, I now have a cloned voice.\"\n",
    "\n",
    "# Init Xtts and load config object\n",
    "cfg = XttsConfig()\n",
    "cfg.load_json(config_path)\n",
    "\n",
    "\n",
    "# Init model and load from checkpoint\n",
    "model = Xtts.init_from_config(cfg)\n",
    "\n",
    "model.load_checkpoint(\n",
    "    cfg,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    vocab_path=vocab_path,\n",
    "    eval=True, \n",
    ")\n",
    "\n",
    "# Set to eval\n",
    "model.to(device).eval()\n",
    "\n",
    "# Get the gpt conditonal latent codes and speaker_encoder from the reference audio mel spectrogram\n",
    "gpt_cond_latent, speaker_encoder = model.get_conditioning_latents(\n",
    "    audio_path=[speaker_ref], # Speaker reference wav pointed here. Multiple can be used. Important for quality of output\n",
    "    gpt_cond_len=cfg.gpt_cond_len, #  Context window size of latents being passed to GPT \n",
    "    gpt_cond_chunk_len=cfg.gpt_cond_chunk_len, # How many chunks audio tokens split into before going to PercieverResampler\n",
    "    max_ref_length=cfg.max_ref_len, # Limits how much of the speaker reference audio is used.\n",
    ")\n",
    "\n",
    "\n",
    "# Model's inference method\n",
    "output = model.inference(\n",
    "            text=text, # Input text\n",
    "            language=\"en\", # Set language to english \n",
    "            gpt_cond_latent=gpt_cond_latent, # Pass conditional latents to GPT\n",
    "            speaker_embedding=speaker_encoder, # Pass Speaker Encoder to Decoder  \n",
    "            temperature=0.75,\n",
    "            speed=1,\n",
    "            length_penalty=cfg.length_penalty,\n",
    "            repetition_penalty=cfg.repetition_penalty,\n",
    "            top_k=cfg.top_k,\n",
    "            top_p=cfg.top_p,\n",
    "        )\n",
    "\n",
    "# Create wav tensor\n",
    "wav_tensor = torch.tensor(out[\"wav\"]).unsqueeze(0)  # shape: (1, samples)\n",
    "\n",
    "# Save tensor output to audio file using torchaudio\n",
    "torchaudio.save(output_path, wav_tensor, sample_rate=cfg.audio.output_sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca6030",
   "metadata": {},
   "source": [
    "<center><h1> Data Preparation for Fine-Tuning XTTS </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc2200",
   "metadata": {},
   "source": [
    "# <img src=\"img/workflow.png\" width=600 height=500 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts_project_venv",
   "language": "python",
   "name": "xtts_project_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
